{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cb5mYRjI8hav"
      },
      "source": [
        "## 第一题"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIhTa_g68SH4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "def softmax(x):\n",
        "  x = np.array(x)\n",
        "  max_x = np.max(x)\n",
        "  return np.exp(x-max_x) / np.sum(np.exp(x-max_x))\n",
        "\n",
        "class myRNN:\n",
        "  def __init__(self, data_dim, hidden_dim=100, bptt_back=4):\n",
        "    # data_dim: 词向量维度，即词典长度; hidden_dim: 隐单元维度; bptt_back: 反向传播回传时间长度\n",
        "    self.data_dim = data_dim\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.bptt_back = bptt_back\n",
        "\n",
        "    # 初始化权重向量 U， W， V; U为输入权重; W为递归权重; V为输出权重\n",
        "    self.U = np.random.uniform(-np.sqrt(1.0/self.data_dim), np.sqrt(1.0/self.data_dim),\n",
        "                                (self.hidden_dim, self.data_dim))\n",
        "    self.W = np.random.uniform(-np.sqrt(1.0/self.hidden_dim), np.sqrt(1.0/self.hidden_dim),\n",
        "                                (self.hidden_dim, self.hidden_dim))\n",
        "    self.V = np.random.uniform(-np.sqrt(1.0/self.hidden_dim), np.sqrt(1.0/self.hidden_dim),\n",
        "                                (self.data_dim, self.hidden_dim))\n",
        "\n",
        "  # 前向传播\n",
        "  def forward(self, x):\n",
        "    # 向量时间长度\n",
        "    T = len(x)\n",
        "\n",
        "    # 初始化状态向量, s包含额外的初始状态 s[-1]\n",
        "    s = np.zeros((T+1, self.hidden_dim))\n",
        "    o = np.zeros((T, self.data_dim))\n",
        "\n",
        "    for t in range(T): #xrange\n",
        "      s[t] = np.tanh(self.U[:, x[t]] + self.W.dot(s[t-1]))\n",
        "      o[t] = softmax(self.V.dot(s[t]))\n",
        "\n",
        "    return [o, s]\n",
        "\n",
        "  # 预测输出\n",
        "  def predict(self, x):\n",
        "      o, s = self.forward(x)\n",
        "      pre_y = np.argmax(o, axis=1)\n",
        "      return pre_y\n",
        "\n",
        "  # 计算损失， softmax损失函数， (x,y)为多个样本\n",
        "  def loss(self, x, y):\n",
        "    cost = 0\n",
        "    for i in range(len(y)):\n",
        "      o, s = self.forward(x[i])\n",
        "      # 取出 y[i] 中每一时刻对应的预测值\n",
        "      pre_yi = o[range(len(y[i])), y[i]]\n",
        "      cost -= np.sum(np.log(pre_yi))\n",
        "\n",
        "    # 统计所有y中词的个数, 计算平均损失\n",
        "    N = np.sum([len(yi) for yi in y])\n",
        "    ave_loss = cost / N\n",
        "\n",
        "    return ave_loss\n",
        "\n",
        "  # 求梯度, (x,y)为一个样本\n",
        "  def bptt(self, x, y):\n",
        "    dU = np.zeros(self.U.shape)\n",
        "    dW = np.zeros(self.W.shape)\n",
        "    dV = np.zeros(self.V.shape)\n",
        "\n",
        "    o, s = self.forward(x)\n",
        "    delta_o = o\n",
        "    delta_o[range(len(y)), y] -= 1\n",
        "\n",
        "    for t in np.arange(len(y))[::-1]:\n",
        "      # 梯度沿输出层向输入层的传播\n",
        "      dV += delta_o[t].reshape(-1, 1) * s[t].reshape(1, -1)  # self.data_dim * self.hidden_dim\n",
        "      delta_t = delta_o[t].reshape(1, -1).dot(self.V) * ((1 - s[t-1]**2).reshape(1, -1)) # 1 * self.hidden_dim\n",
        "\n",
        "      # 梯度沿时间t的传播\n",
        "      for bpt_t in np.arange(np.max([0, t-self.bptt_back]), t+1)[::-1]:\n",
        "        dW += delta_t.T.dot(s[bpt_t-1].reshape(1, -1))\n",
        "        dU[:, x[bpt_t]] = dU[:, x[bpt_t]] + delta_t\n",
        "\n",
        "        delta_t = delta_t.dot(self.W.T) * (1 - s[bpt_t-1]**2)\n",
        "\n",
        "    return [dU, dW, dV]\n",
        "\n",
        "  # 计算梯度\n",
        "  def sgd_step(self, x, y, learning_rate):\n",
        "    dU, dW, dV = self.bptt(x, y)\n",
        "\n",
        "    self.U -= learning_rate * dU\n",
        "    self.W -= learning_rate * dW\n",
        "    self.V -= learning_rate * dV\n",
        "\n",
        "  # 训练RNN\n",
        "  def train(self, X_train, y_train, learning_rate=0.005, n_epoch=5):\n",
        "    losses = []\n",
        "    num_examples = 0\n",
        "\n",
        "    for epoch in range(n_epoch):\n",
        "      for i in range(len(y_train)):\n",
        "        self.sgd_step(X_train[i], y_train[i], learning_rate)\n",
        "        num_examples += 1\n",
        "\n",
        "      loss = self.loss(X_train, y_train)\n",
        "      losses.append(loss)\n",
        "      print('epoch {0}: loss = {1}'.format(epoch+1, loss))\n",
        "      # 若损失增加，降低学习率\n",
        "      if len(losses) > 1 and losses[-1] > losses[-2]:\n",
        "        learning_rate *= 0.5\n",
        "        print('decrease learning_rate to', learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XsCL_8XcAH2h"
      },
      "outputs": [],
      "source": [
        "rnn = myRNN(4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FbLY3Xif8gxx"
      },
      "outputs": [],
      "source": [
        "X = np.array([\n",
        "    [1,0,0,0],\n",
        "    [0,1,0,0],\n",
        "    [0,0,1,0],\n",
        "    [0,0,0,1],\n",
        "    [1, 0, 0, 0],\n",
        "    [0, 1, 0, 0],\n",
        "    [0, 0, 1, 0],\n",
        "    [0, 0, 0, 1],\n",
        "    [1, 0, 0, 0],\n",
        "    [0, 1, 0, 0],\n",
        "    [0, 0, 1, 0],\n",
        "    [0, 0, 0, 1]\n",
        "])\n",
        "y =  np.array([\n",
        "    [0,1,0,0],\n",
        "    [0,0,1,0],\n",
        "    [0,0,0,1],\n",
        "    [0,0,0,0],\n",
        "    [0, 1, 0, 0],\n",
        "    [0, 0, 1, 0],\n",
        "    [0, 0, 0, 1],\n",
        "    [0, 0, 0, 0],\n",
        "    [0, 1, 0, 0],\n",
        "    [0, 0, 1, 0],\n",
        "    [0, 0, 0, 1],\n",
        "    [0, 0, 0, 0],\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qA_rivDP8g0m",
        "outputId": "80cdddcd-fd69-47de-9e3a-cbcbd9d0c1e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 1: loss = 0.8700298718487586\n",
            "epoch 2: loss = 0.6327577010423717\n",
            "epoch 3: loss = 0.5063252751893155\n",
            "epoch 4: loss = 0.4178235284559139\n",
            "epoch 5: loss = 0.348172729305062\n"
          ]
        }
      ],
      "source": [
        "rnn.train(X,y)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# 定义RNN类\n",
        "class RNN:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        # 初始化权重\n",
        "        self.Wxh = np.random.randn(hidden_size, input_size) * 0.01\n",
        "        self.Whh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
        "        self.Why = np.random.randn(output_size, hidden_size) * 0.01\n",
        "        self.bh = np.zeros((hidden_size, 1))\n",
        "        self.by = np.zeros((output_size, 1))\n",
        "        # 存储激活值\n",
        "        self.h = np.zeros((hidden_size, 1))\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        xs, hs, ys, ps = {}, {}, {}, {}\n",
        "        hs[-1] = np.copy(self.h)\n",
        "        # 前向传播\n",
        "        for t, x in enumerate(inputs):\n",
        "            xs[t] = np.zeros((input_size, 1))\n",
        "            xs[t][x] = 1  # 将输入编码为 one-hot 向量\n",
        "            hs[t] = np.tanh(np.dot(self.Wxh, xs[t]) + np.dot(self.Whh, hs[t-1]) + self.bh)  # 隐藏状态\n",
        "            ys[t] = np.dot(self.Why, hs[t]) + self.by  # 输出\n",
        "            ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t]))  # 概率分布\n",
        "\n",
        "        return ps, hs\n",
        "\n",
        "    def sample(self, seed, n):\n",
        "        # 从初始种子字符开始生成新文本\n",
        "        x = seed\n",
        "        # 将种子字符编码为 one-hot 向量\n",
        "        x_one_hot = np.zeros((input_size, 1))\n",
        "        x_one_hot[x] = 1\n",
        "        # 存储生成的文本\n",
        "        generated_text = [x]\n",
        "\n",
        "        # 前向传播，生成文本\n",
        "        for _ in range(n):\n",
        "            _, h = self.forward([x])\n",
        "            # 计算下一个字符的概率分布\n",
        "            p = h[-1]\n",
        "            # 根据概率分布随机选择下一个字符\n",
        "            x = np.random.choice(range(output_size), p=p.ravel())\n",
        "            # 添加到生成的文本中\n",
        "            generated_text.append(x)\n",
        "        \n",
        "        return generated_text\n",
        "\n",
        "# 输入输出大小\n",
        "input_size = output_size = 256\n",
        "# 隐藏层大小\n",
        "hidden_size = 128\n",
        "\n",
        "# 创建RNN对象\n",
        "rnn = RNN(input_size, hidden_size, output_size)\n",
        "\n",
        "# 用户输入文本\n",
        "user_input = input(\"请输入文本：\")\n",
        "\n",
        "# 将文本转换为字符索引\n",
        "char_to_idx = {chr(i): i for i in range(256)}\n",
        "idx_to_char = {i: chr(i) for i in range(256)}\n",
        "input_text = [char_to_idx[char] for char in user_input]\n",
        "\n",
        "# 生成续写的文本\n",
        "generated_text = rnn.sample(input_text[-1], 500)\n",
        "\n",
        "# 将生成的文本转换为字符串\n",
        "generated_text = ''.join([idx_to_char[idx] for idx in generated_text])\n",
        "\n",
        "# 输出"
      ],
      "metadata": {
        "id": "rH1TXV1wQBuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDjJYdaW8Svg"
      },
      "source": [
        "## 第二题"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "Ygt_n2evSDMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Loss():\n",
        "  def forward(self,y_hat,y):\n",
        "    delta=1e-7  #添加一个微小值可以防止负无限大(np.log(0))的发生。\n",
        "    return -np.sum(y_hat*np.log(y+delta))\n",
        "    pass\n",
        "  def backward(self,y_hat,y):\n",
        "    delta=1e-7\n",
        "    return np.log(y+delta)\n",
        "    pass\n",
        "  pass"
      ],
      "metadata": {
        "id": "imZmSj-9R3Af"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Tanh():\n",
        "\n",
        "  # 函数原型：e^x/sum(e^x)\n",
        "  def forward(self, input):\n",
        "    '''\n",
        "    正向计算求值\n",
        "    :param input:\n",
        "    :return:\n",
        "    '''\n",
        "    result = np.exp(input)\n",
        "    result2 = np.exp(-input)\n",
        "    return (result - result2) / (result + result2)\n",
        "    pass\n",
        "\n",
        "  def backward(self, output):\n",
        "    '''\n",
        "    反向计算求梯度\n",
        "    :param output: 是正向计算的结果，forward的计算结果\n",
        "    :return:\n",
        "    '''\n",
        "    return 1 - output ** 2\n",
        "    pass\n",
        "  pass"
      ],
      "metadata": {
        "id": "LRXd9eC8R5xH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SoftMax():\n",
        "# 函数原型：e^x/sum(e^x)\n",
        "  def forward(self, input):\n",
        "    '''\n",
        "    正向计算求值\n",
        "    :param input:\n",
        "    :return:\n",
        "    '''\n",
        "    result = np.exp(input)\n",
        "    return result / (np.sum(result) + 0.001)\n",
        "    pass\n",
        "\n",
        "  def backward(self, output):\n",
        "    '''\n",
        "    反向计算求梯度\n",
        "    :param output: 是正向计算的结果，forward的计算结果\n",
        "    :return:\n",
        "    '''\n",
        "    return output * (1 - output)\n",
        "    pass\n",
        "  pass"
      ],
      "metadata": {
        "id": "6qwQ-DJNR9rx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Relu():\n",
        "  def forward(self,input):\n",
        "    \"\"\"\n",
        "    正向计算求值\n",
        "    \"\"\"\n",
        "    result = np.maximum(0,input)\n",
        "    return result\n",
        "    pass\n",
        "\n",
        "  def backward(self,output):\n",
        "    \"\"\"\n",
        "    反向传播计算梯度\n",
        "    \"\"\"\n",
        "    output[output>0] = 1\n",
        "    output[output<=0] = 0\n",
        "    return output\n",
        "    pass\n",
        "  pass"
      ],
      "metadata": {
        "id": "7xP4Da-hSB_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WRNN():\n",
        "  #初始化操作\n",
        "  def __init__(self,input_size,state_size,output_size,times=1,maxlen=32,learningRate = 0.01):\n",
        "    \"\"\"\n",
        "    定义网络结构\n",
        "    input_size:输入x向量的长度\n",
        "    state_size:隐藏层b的长度，隐藏层a的长度：state_size+2\n",
        "    output_size:输出y向量的长度\n",
        "    time:记录时间状态\n",
        "\n",
        "    \"\"\"\n",
        "    self.input_size = input_size\n",
        "    self.output_size = output_size\n",
        "    self.hidden_a_size = state_size\n",
        "    self.state_size  = state_size \n",
        "    self.hidden_b_size = state_size\n",
        "    #self.activator = activator\n",
        "\n",
        "    #初始化权重矩阵\n",
        "    self.Ux = np.random.uniform(-0.5, 0.5, (self.state_size, input_size))\n",
        "    self.Vx = np.random.uniform(-0.5, 0.5, (self.state_size, self.state_size))\n",
        "    self.Qx = np.random.uniform(-0.5, 0.5, (self.output_size, self.state_size))\n",
        "    self.Rx = np.random.uniform(-0.5, 0.5, (self.state_size, self.state_size))\n",
        "    self.Ws = np.random.uniform(-0.5, 0.5, (self.state_size, self.state_size))\n",
        "    self.Tx = np.random.uniform(-0.5, 0.5, (self.state_size, self.state_size))\n",
        "\n",
        "    #初始化bias\n",
        "    self.S1 = np.zeros(self.state_size)\n",
        "    self.S2 = np.zeros(self.state_size)\n",
        "    self.S3 = np.zeros(self.output_size)\n",
        "    # 记录状态信息\n",
        "    self.stateList_a = []      # 有少条样本就有产生多少条状态\n",
        "    self.stateList_b = []      # 有少条样本就有产生多少条状态\n",
        "    self.stateList_ha = []\n",
        "    self.stateList_hb = []\n",
        "    self.times = times       # times是记录的计算次数，一个轮次内，最大是系列总长度\n",
        "    self.maxLen = maxlen     # 暂时不用吧\n",
        "    self.outputList = []\n",
        "\n",
        "    #记录梯度，后续方便实现不同的优化器SGDM、Adam等\n",
        "    self.S1GradList = []\n",
        "    self.S2GradList = []\n",
        "    self.S3GradList = []\n",
        "\n",
        "    self.UxGradList = []\n",
        "    self.VxGradList = []\n",
        "    self.QxGradList = []\n",
        "    self.RxGradList = []\n",
        "    self.WsGradList = []\n",
        "    self.TxGradList = []\n",
        "    \n",
        "  #t时刻前向传播的计算\n",
        "  def forward(self,inputX,times,maxLen):\n",
        "    if times - 1 == 0: # t0时刻对stateList，outputList重置，说明一轮训练结束\n",
        "      self.stateList_a = []\n",
        "      self.stateList_b = []\n",
        "      self.outputList = []\n",
        "      self.stateList_ha = []\n",
        "      self.stateList_hb = []\n",
        "      self.stateList_b.append(np.zeros(self.state_size))\n",
        "      self.stateList_hb.append(np.zeros(self.state_size))#初始增加bt-1，全零值，方便统一计算\n",
        "      self.outputList.append(np.zeros(self.output_size))\n",
        "      self.stateList_a.append(np.zeros(self.state_size)) \n",
        "      self.stateList_ha.append(np.zeros(self.state_size))\n",
        "      #t=1时，state_a中无数据，随机初始化一个a_t-2\n",
        "      #self.stateList_a.append(np.random.uniform(0,1,self.stateSize))\n",
        "      #self.stateList_b.append = []\n",
        "      #清空历史梯度\n",
        "      self.S1GradList = []\n",
        "      self.S2GradList = []\n",
        "      self.S3GradList = []\n",
        "\n",
        "      self.UxGradList = []\n",
        "      self.VxGradList = []\n",
        "      self.QxGradList = []\n",
        "      self.RxGradList = []\n",
        "      self.WsGradList = []\n",
        "      self.TxGradList = []\n",
        "      \n",
        "    state_a_ = self.stateList_a[-1]#t-1时刻的state_a\n",
        "    #t时刻state_a的计算，使用tanh作为激活函数\n",
        "    state_a = np.dot(self.Ux,inputX)+np.dot(self.Ws,state_a_)+self.S1\n",
        "    self.stateList_ha.append(state_a)\n",
        "    tanh = Tanh()##f\n",
        "    state_a = tanh.forward(state_a)\n",
        "    self.stateList_a.append(state_a)\n",
        "\n",
        "\n",
        "    state_b_ = self.stateList_b[-1]\n",
        "    #t时刻state_b的计算，使用relu作为激活函数\n",
        "    #########################################\n",
        "    state_b = np.dot(self.Vx,state_a)+np.dot(self.Rx,state_a_)+np.dot(self.Tx,state_b_)+self.S2\n",
        "    self.stateList_hb.append(state_b)\n",
        "    #relu = Relu()\n",
        "    state_b = tanh.forward(state_b)\n",
        "    self.stateList_b.append(state_b)\n",
        "    #t时刻的输出值，使用softmax作为激活函数\n",
        "    output = np.dot(self.Qx,state_b)+self.S3\n",
        "    softmax = SoftMax()\n",
        "    output = softmax.forward(output)\n",
        "    # 记录t时刻的输出\n",
        "    self.outputList.append(output)\n",
        "\n",
        "    self.times = times # 下一个时刻\n",
        "    return output\n",
        "    \n",
        "  \n",
        "\n",
        "  #反向传播求梯度\n",
        "  # loss函数使用 E(y) = -np.sum(y * log(y) + (1 - y)*log)\n",
        "  def backward(self,X,inputY,T):\n",
        "    #deltaT = self.Qx.dot(self.outputList[T] - inputY[T-1])\n",
        "    #deltaList = [0 for i in range(0, T+1)]\n",
        "    #deltaList[T] = deltaT\n",
        "    g = SoftMax()\n",
        "    f = Tanh()\n",
        "    loss = Loss()\n",
        "    #初始化梯度为0\n",
        "    #loss/S1 loss/S2 loss/S3\n",
        "    self.S1Grad = np.zeros(shape = self.S1.shape)\n",
        "    self.S2Grad = np.zeros(shape = self.S2.shape)\n",
        "    self.S3Grad = np.zeros(shape = self.S3.shape)\n",
        "    #loss/Q loss/R loss/U loss/V loss/W\n",
        "    self.UxGrad = np.zeros(shape = self.Ux.shape)\n",
        "    self.VxGrad = np.zeros(shape = self.Vx.shape)\n",
        "    self.QxGrad = np.zeros(shape = self.Qx.shape)\n",
        "    self.RxGrad = np.zeros(shape = self.Rx.shape)\n",
        "    self.WsGrad = np.zeros(shape = self.Ws.shape)\n",
        "    self.TxGrad = np.zeros(shape = self.Tx.shape)\n",
        "    \n",
        "    for t in range(1,T+1):\n",
        "      o_b_Grad = np.dot(g.forward(np.dot(self.Qx,self.stateList_b[t])),self.Qx)\n",
        "      b_s_Grad = f.forward(self.stateList_hb[t])\n",
        "      b_V_Grad = np.dot(b_s_Grad,self.Vx)\n",
        "      b_R_Grad = np.dot(b_s_Grad,np.dot(self.stateList_a[t-1].T,np.eye(self.Vx.shape[0],self.Vx.shape[1])))\n",
        "      \n",
        "      b_a_Grad = np.dot(f.backward(self.stateList_ha[t]),self.Vx)\n",
        "      b_a1_Grad =  np.dot(f.backward(self.stateList_ha[t]),self.Rx)\n",
        "      a_p_Grad = f.forward(self.stateList_ha[t])\n",
        "      a1_p_Grad = f.forward(self.stateList_ha[t-1])\n",
        "      l_o_Grad = np.dot(loss.backward(self.outputList[t],inputY[t-1]), self.stateList_b[t]) \n",
        "      a_U_Grad = np.dot(f.forward(self.stateList_ha[t]),np.dot(X.T,np.eye(self.Ux.shape[0],self.Ux.shape[1])))\n",
        "     \n",
        "      #a1_U_Grad = f.forward(self.stateList_ha[t-1])\n",
        "      a_W_Grad = np.dot(f.forward(self.stateList_ha[t]),np.dot(self.stateList_a[t].T,np.eye(self.Wx.shape[0],self.Wx.shape[1])))\n",
        "      #a1_W_Grad = f.forward(self.stateList_ha[t-1])\n",
        "     \n",
        "      self.SGrad += np.dot(np.dot(loss.backward(self.outputList[t] , inputY[t-1]),o_b_Grad), b_s_Grad)\n",
        "      self.VxGrad +=  np.dot(np.dot(loss.backward(self.outputList[t] , inputY[t-1])[:, np.newaxis],o_b_Grad[np.newaxis,:])[:, np.newaxis], b_V_Grad[np.newaxis,:])\n",
        "      self.RxGrad +=  np.dot(np.dot(loss.backward(self.outputList[t] , inputY[t-1])[:, np.newaxis],o_b_Grad[np.newaxis,:])[:, np.newaxis], b_R_Grad[np.newaxis,:])\n",
        "      self.PGrad += np.dot(np.dot(np.dot(l_o_Grad ,o_b_Grad ),b_a_Grad),a_p_Grad) + np.dot(np.dot(np.dot(l_o_Grad ,o_b_Grad ),b_a1_Grad),a1_p_Grad)\n",
        "      self.WxGrad += np.dot(np.dot(np.dot(l_o_Grad ,o_b_Grad ),b_a_Grad),a_U_Grad)\n",
        "      self.UxGrad += np.dot(np.dot(np.dot(l_o_Grad ,o_b_Grad ),b_a_Grad),a_W_Grad)\n",
        "      self.TxGrad += np.dot(np.dot(np.dot(l_o_Grad ,o_b_Grad ),b_a_Grad),a_W_Grad)\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "    #记录梯度\n",
        "    self.S1GradList.append(self.S1Grad)\n",
        "    self.S2GradList.append(self.S2Grad)\n",
        "    self.S3GradList.append(self.S3Grad)\n",
        "\n",
        "    self.UxGradList.append(self.UxGrad)\n",
        "    self.VxGradList.append(self.VxGrad)\n",
        "    self.QxGradList.append(self.QxGrad)\n",
        "    self.RxGradList.append(self.RxGrad)\n",
        "    self.WsGradList.append(self.WsGrad)\n",
        "    self.TxGradList.append(self.TxGrad)\n",
        "    \n",
        "  \n",
        "\n",
        "  def update(self,learningRate):\n",
        "\n",
        "    self.Ux -= self.UxGrad*learningRate\n",
        "    self.Vx -= self.VxGrad*learningRate\n",
        "    self.Qx -= self.QxGrad*learningRate\n",
        "    self.Rx -= self.RxGrad*learningRate\n",
        "    self.Ws -= self.WsGrad*learningRate\n",
        "    self.Tx -= self.TxGrad*learningRate\n",
        "    \n",
        "    self.S1 -= self.S1Grad*learningRate\n",
        "    self.S2 -= self.S2Grad*learningRate\n",
        "    self.S3 -= self.S3Grad*learningRate\n",
        "    pass\n",
        "\n",
        "\n",
        "  def predict(self, input):\n",
        "    \n",
        "    output = self.forward(input, 1, 12)\n",
        "\n",
        "    return output\n",
        "    pass\n",
        "\n",
        "  def fit(self, X, Y, loss=None, epochs=100, learningRate=0.1):\n",
        "    '''\n",
        "    训练模型\n",
        "    :param X:\n",
        "    :param y:\n",
        "    :param loss: 成本函数\n",
        "    :return:\n",
        "    '''\n",
        "    T = len(X)\n",
        "    for i in range(epochs):\n",
        "      # epochs\n",
        "      print(\"epochs:\", i)\n",
        "      times = 1\n",
        "      # 正向计算所有的state和output\n",
        "      for x, y in zip(X, Y):\n",
        "        self.forward(x, times, T)  # T是系列的总长度，暂时不考虑截取\n",
        "        times += 1\n",
        "        pass\n",
        "      # 反向传播求梯度\n",
        "      self.backward(X, Y, T)\n",
        "      # 跟新梯度\n",
        "      self.update(learningRate) \n",
        "    pass\n",
        "  pass\n"
      ],
      "metadata": {
        "id": "32SOYhQeRQkv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EMhzffAJewbL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "class WRNN2():\n",
        "  #初始化操作\n",
        "  def __init__(self,input_size,state_size,output_size,times=1,maxlen=32,learningRate = 0.01):\n",
        "    \"\"\"\n",
        "    定义网络结构\n",
        "    input_size:输入x向量的长度\n",
        "    state_size:隐藏层b的长度，隐藏层a的长度：state_size+2\n",
        "    output_size:输出y向量的长度\n",
        "    time:记录时间状态\n",
        "\n",
        "    \"\"\"\n",
        "    self.input_size = input_size\n",
        "    self.output_size = output_size\n",
        "    self.hidden_a_size = state_size\n",
        "    self.state_size  = state_size \n",
        "    self.hidden_b_size = state_size\n",
        "    #self.activator = activator\n",
        "\n",
        "    #初始化权重矩阵\n",
        "    self.Ux = torch.normal(-0.5, 0.5, size = (self.state_size, input_size),requires_grad=True)\n",
        "    self.Vx = torch.normal(-0.5, 0.5, size = (self.state_size, self.state_size),requires_grad=True)\n",
        "    self.Qx = torch.normal(-0.5, 0.5, size = (self.output_size, self.state_size),requires_grad=True)\n",
        "    self.Rx = torch.normal(-0.5, 0.5, size = (self.state_size, self.state_size),requires_grad=True)\n",
        "    self.Ws = torch.normal(-0.5, 0.5, size = (self.state_size, self.state_size),requires_grad=True)\n",
        "    self.Tx = torch.normal(-0.5, 0.5, size = (self.state_size, self.state_size),requires_grad=True)\n",
        "\n",
        "    #初始化bias\n",
        "    self.S1 = torch.zeros(self.state_size,requires_grad=True)\n",
        "    self.S2 = torch.zeros(self.state_size,requires_grad=True)\n",
        "    self.S3 = torch.zeros(self.output_size,requires_grad=True)\n",
        "    # 记录状态信息\n",
        "    self.stateList_a = []      # 有少条样本就有产生多少条状态\n",
        "    self.stateList_b = []      # 有少条样本就有产生多少条状态\n",
        "    self.stateList_ha = []\n",
        "    self.stateList_hb = []\n",
        "    self.times = times       # times是记录的计算次数，一个轮次内，最大是系列总长度\n",
        "    self.maxLen = maxlen     # 暂时不用吧\n",
        "    self.outputList = []\n",
        "\n",
        "    self.softmax = torch.nn.Softmax(dim=0)\n",
        "    self.relu = torch.nn.ReLU()##f\n",
        "  #t时刻前向传播的计算\n",
        "  def forward(self,inputX,times,maxLen=0):\n",
        "    if times - 1 == 0: # t0时刻对stateList，outputList重置，说明一轮训练结束\n",
        "      self.stateList_a = []\n",
        "      self.stateList_b = []\n",
        "      self.outputList = []\n",
        "      self.stateList_ha = []\n",
        "      self.stateList_hb = []\n",
        "      self.stateList_b.append(torch.zeros(self.state_size,requires_grad=True))\n",
        "      self.stateList_hb.append(torch.zeros(self.state_size,requires_grad=True))#初始增加bt-1，全零值，方便统一计算\n",
        "      #self.outputList.append(torch.zeros(self.output_size,requires_grad=True))\n",
        "      self.stateList_a.append(torch.zeros(self.state_size,requires_grad=True)) \n",
        "      self.stateList_ha.append(torch.zeros(self.state_size,requires_grad=True))\n",
        "      #t=1时，state_a中无数据，随机初始化一个a_t-2\n",
        "      #self.stateList_a.append(np.random.uniform(0,1,self.stateSize))\n",
        "      #self.stateList_b.append = []\n",
        "      \n",
        "    state_a_ = self.stateList_a[-1]#t-1时刻的state_a\n",
        "    #t时刻state_a的计算，使用tanh作为激活函数\n",
        "    state_a = self.Ux@inputX + self.Ws@state_a_ + self.S1\n",
        "    self.stateList_ha.append(state_a)\n",
        "    \n",
        "    state_a = self.relu(state_a)\n",
        "    self.stateList_a.append(state_a)\n",
        "\n",
        "\n",
        "    state_b_ = self.stateList_b[-1]\n",
        "    #t时刻state_b的计算，使用relu作为激活函数\n",
        "    #########################################\n",
        "    state_b = self.Vx@state_a + self.Rx@state_a_ + self.Tx@state_b_ + self.S2\n",
        "    self.stateList_hb.append(state_b)\n",
        "    #relu = Relu()\n",
        "    state_b = self.relu(state_b)\n",
        "    self.stateList_b.append(state_b)\n",
        "    #t时刻的输出值，使用softmax作为激活函数\n",
        "    output = self.Qx@state_b+self.S3\n",
        "    \n",
        "    output = self.softmax(output)\n",
        "    # 记录t时刻的输出\n",
        "    self.outputList.append(output)\n",
        "\n",
        "    self.times = times # 下一个时刻\n",
        "    return output\n",
        "\n",
        "  def get_parameters(self):\n",
        "    return [self.S1,self.S2,self.S3,self.Ux,self.Vx,self.Qx,self.Rx,self.Ws,self.Tx]\n",
        "    \n",
        "  def predict(self,X):\n",
        "    \n",
        "    state_a_List = []\n",
        "    state_b_list = []\n",
        "    output_list =[]\n",
        "    state_a_List.append(self.stateList_a[-1])\n",
        "    state_b_list.append(torch.zeros(self.state_size,requires_grad=True))\n",
        "    for i in range(X.shape[0]):\n",
        "      state_a_ = state_a_List[-1]#t-1时刻的state_a\n",
        "      state_a = self.Ux@X[i] + self.Ws@state_a_ + self.S1\n",
        "      \n",
        "      state_a = self.relu(state_a)\n",
        "      state_a_List.append(state_a)\n",
        "\n",
        "\n",
        "      state_b_ = state_b_list[-1]\n",
        "      #t时刻state_b的计算，使用relu作为激活函数\n",
        "      #########################################\n",
        "      state_b = self.Vx@state_a + self.Rx@state_a_ + self.Tx@state_b_ + self.S2\n",
        "      \n",
        "      state_b = self.relu(state_b)\n",
        "      state_b_list.append(state_b)\n",
        "      #t时刻的输出值，使用softmax作为激活函数\n",
        "      output = self.Qx@state_b+self.S3\n",
        "      \n",
        "      output = self.softmax(output)\n",
        "      # 记录t时刻的输出\n",
        "      output_list.append(output)\n",
        "\n",
        "      \n",
        "    return output_list\n",
        "     \n",
        "    \n",
        "  def fit(self, X, Y, loss=None, epochs=10, learningRate=0.01):\n",
        "    '''\n",
        "    训练模型\n",
        "    :param X:\n",
        "    :param y:\n",
        "    :param loss: 成本函数\n",
        "    :return:\n",
        "    '''\n",
        "    optimizer = torch.optim.SGD(self.get_parameters(),learningRate)\n",
        "    T = len(X)\n",
        "    if loss is None:\n",
        "      loss = torch.nn.CrossEntropyLoss()\n",
        "    \n",
        "    \n",
        "    for i in range(epochs):\n",
        "      # epochs\n",
        "      loss_sum = 0\n",
        "      print(\"epochs:\", i)\n",
        "      times = 1\n",
        "      # 正向计算所有的state和output\n",
        "      for x, y in zip(X, Y):\n",
        "        y_hat = self.forward(x, times, T)  # T是系列的总长度，暂时不考虑截取\n",
        "        self.outputList.append(y_hat)\n",
        "        times += 1\n",
        "        loss_sum += loss(y_hat,y) #计算每次的损失，并累加\n",
        "        \n",
        "      print(\"loss:\",loss_sum)\n",
        "      # 跟新梯度\n",
        "      optimizer.zero_grad()\n",
        "      loss_sum.backward()\n",
        "      optimizer.step()\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ecp7zPrZe_jC"
      },
      "outputs": [],
      "source": [
        "rnn=WRNN2(4, 10 , 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXf3YDJu1CsD"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOnAQ3rP1Fzq",
        "outputId": "79eadf40-98b3-424c-a4c8-f8261c8fd2bf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-67-0ba13b5a78f2>:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  y = torch.tensor(y)\n"
          ]
        }
      ],
      "source": [
        "X = torch.tensor([\n",
        "    [3.,1.,0.,0.],\n",
        "    [1.,0.,1.,0.],\n",
        "    [1.,0.,0.,1.],\n",
        "    [0.,1.,0.,0.]\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lpFUm-uxgSEt"
      },
      "outputs": [],
      "source": [
        "Y = torch.tensor([\n",
        "    [1.,0.,0.,0.],\n",
        "    [0.,1.,0.,0.],\n",
        "    [0.,0.,1.,0.],\n",
        "    [0.,0.,0.,1.]\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uko8qAGs1PdK",
        "outputId": "13e56348-c6b0-4087-89a5-6e942753485c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epochs: 0\n",
            "loss: tensor(4.7687, grad_fn=<AddBackward0>)\n",
            "epochs: 1\n",
            "loss: tensor(4.7664, grad_fn=<AddBackward0>)\n",
            "epochs: 2\n",
            "loss: tensor(4.7641, grad_fn=<AddBackward0>)\n",
            "epochs: 3\n",
            "loss: tensor(4.7618, grad_fn=<AddBackward0>)\n",
            "epochs: 4\n",
            "loss: tensor(4.7598, grad_fn=<AddBackward0>)\n",
            "epochs: 5\n",
            "loss: tensor(4.7573, grad_fn=<AddBackward0>)\n",
            "epochs: 6\n",
            "loss: tensor(4.7549, grad_fn=<AddBackward0>)\n",
            "epochs: 7\n",
            "loss: tensor(4.7524, grad_fn=<AddBackward0>)\n",
            "epochs: 8\n",
            "loss: tensor(4.7500, grad_fn=<AddBackward0>)\n",
            "epochs: 9\n",
            "loss: tensor(4.7475, grad_fn=<AddBackward0>)\n",
            "epochs: 10\n",
            "loss: tensor(4.7450, grad_fn=<AddBackward0>)\n",
            "epochs: 11\n",
            "loss: tensor(4.7426, grad_fn=<AddBackward0>)\n",
            "epochs: 12\n",
            "loss: tensor(4.7403, grad_fn=<AddBackward0>)\n",
            "epochs: 13\n",
            "loss: tensor(4.7377, grad_fn=<AddBackward0>)\n",
            "epochs: 14\n",
            "loss: tensor(4.7351, grad_fn=<AddBackward0>)\n",
            "epochs: 15\n",
            "loss: tensor(4.7325, grad_fn=<AddBackward0>)\n",
            "epochs: 16\n",
            "loss: tensor(4.7299, grad_fn=<AddBackward0>)\n",
            "epochs: 17\n",
            "loss: tensor(4.7273, grad_fn=<AddBackward0>)\n",
            "epochs: 18\n",
            "loss: tensor(4.7246, grad_fn=<AddBackward0>)\n",
            "epochs: 19\n",
            "loss: tensor(4.7220, grad_fn=<AddBackward0>)\n",
            "epochs: 20\n",
            "loss: tensor(4.7196, grad_fn=<AddBackward0>)\n",
            "epochs: 21\n",
            "loss: tensor(4.7169, grad_fn=<AddBackward0>)\n",
            "epochs: 22\n",
            "loss: tensor(4.7142, grad_fn=<AddBackward0>)\n",
            "epochs: 23\n",
            "loss: tensor(4.7114, grad_fn=<AddBackward0>)\n",
            "epochs: 24\n",
            "loss: tensor(4.7086, grad_fn=<AddBackward0>)\n",
            "epochs: 25\n",
            "loss: tensor(4.7058, grad_fn=<AddBackward0>)\n",
            "epochs: 26\n",
            "loss: tensor(4.7029, grad_fn=<AddBackward0>)\n",
            "epochs: 27\n",
            "loss: tensor(4.7001, grad_fn=<AddBackward0>)\n",
            "epochs: 28\n",
            "loss: tensor(4.6978, grad_fn=<AddBackward0>)\n",
            "epochs: 29\n",
            "loss: tensor(4.6949, grad_fn=<AddBackward0>)\n",
            "epochs: 30\n",
            "loss: tensor(4.6920, grad_fn=<AddBackward0>)\n",
            "epochs: 31\n",
            "loss: tensor(4.6891, grad_fn=<AddBackward0>)\n",
            "epochs: 32\n",
            "loss: tensor(4.6862, grad_fn=<AddBackward0>)\n",
            "epochs: 33\n",
            "loss: tensor(4.6833, grad_fn=<AddBackward0>)\n",
            "epochs: 34\n",
            "loss: tensor(4.6804, grad_fn=<AddBackward0>)\n",
            "epochs: 35\n",
            "loss: tensor(4.6775, grad_fn=<AddBackward0>)\n",
            "epochs: 36\n",
            "loss: tensor(4.6745, grad_fn=<AddBackward0>)\n",
            "epochs: 37\n",
            "loss: tensor(4.6721, grad_fn=<AddBackward0>)\n",
            "epochs: 38\n",
            "loss: tensor(4.6691, grad_fn=<AddBackward0>)\n",
            "epochs: 39\n",
            "loss: tensor(4.6662, grad_fn=<AddBackward0>)\n",
            "epochs: 40\n",
            "loss: tensor(4.6632, grad_fn=<AddBackward0>)\n",
            "epochs: 41\n",
            "loss: tensor(4.6602, grad_fn=<AddBackward0>)\n",
            "epochs: 42\n",
            "loss: tensor(4.6572, grad_fn=<AddBackward0>)\n",
            "epochs: 43\n",
            "loss: tensor(4.6542, grad_fn=<AddBackward0>)\n",
            "epochs: 44\n",
            "loss: tensor(4.6512, grad_fn=<AddBackward0>)\n",
            "epochs: 45\n",
            "loss: tensor(4.6482, grad_fn=<AddBackward0>)\n",
            "epochs: 46\n",
            "loss: tensor(4.6454, grad_fn=<AddBackward0>)\n",
            "epochs: 47\n",
            "loss: tensor(4.6427, grad_fn=<AddBackward0>)\n",
            "epochs: 48\n",
            "loss: tensor(4.6397, grad_fn=<AddBackward0>)\n",
            "epochs: 49\n",
            "loss: tensor(4.6367, grad_fn=<AddBackward0>)\n",
            "epochs: 50\n",
            "loss: tensor(4.6337, grad_fn=<AddBackward0>)\n",
            "epochs: 51\n",
            "loss: tensor(4.6307, grad_fn=<AddBackward0>)\n",
            "epochs: 52\n",
            "loss: tensor(4.6276, grad_fn=<AddBackward0>)\n",
            "epochs: 53\n",
            "loss: tensor(4.6246, grad_fn=<AddBackward0>)\n",
            "epochs: 54\n",
            "loss: tensor(4.6216, grad_fn=<AddBackward0>)\n",
            "epochs: 55\n",
            "loss: tensor(4.6186, grad_fn=<AddBackward0>)\n",
            "epochs: 56\n",
            "loss: tensor(4.6156, grad_fn=<AddBackward0>)\n",
            "epochs: 57\n",
            "loss: tensor(4.6129, grad_fn=<AddBackward0>)\n",
            "epochs: 58\n",
            "loss: tensor(4.6101, grad_fn=<AddBackward0>)\n",
            "epochs: 59\n",
            "loss: tensor(4.6071, grad_fn=<AddBackward0>)\n",
            "epochs: 60\n",
            "loss: tensor(4.6041, grad_fn=<AddBackward0>)\n",
            "epochs: 61\n",
            "loss: tensor(4.6010, grad_fn=<AddBackward0>)\n",
            "epochs: 62\n",
            "loss: tensor(4.5981, grad_fn=<AddBackward0>)\n",
            "epochs: 63\n",
            "loss: tensor(4.5951, grad_fn=<AddBackward0>)\n",
            "epochs: 64\n",
            "loss: tensor(4.5921, grad_fn=<AddBackward0>)\n",
            "epochs: 65\n",
            "loss: tensor(4.5891, grad_fn=<AddBackward0>)\n",
            "epochs: 66\n",
            "loss: tensor(4.5862, grad_fn=<AddBackward0>)\n",
            "epochs: 67\n",
            "loss: tensor(4.5832, grad_fn=<AddBackward0>)\n",
            "epochs: 68\n",
            "loss: tensor(4.5802, grad_fn=<AddBackward0>)\n",
            "epochs: 69\n",
            "loss: tensor(4.5777, grad_fn=<AddBackward0>)\n",
            "epochs: 70\n",
            "loss: tensor(4.5747, grad_fn=<AddBackward0>)\n",
            "epochs: 71\n",
            "loss: tensor(4.5717, grad_fn=<AddBackward0>)\n",
            "epochs: 72\n",
            "loss: tensor(4.5688, grad_fn=<AddBackward0>)\n",
            "epochs: 73\n",
            "loss: tensor(4.5658, grad_fn=<AddBackward0>)\n",
            "epochs: 74\n",
            "loss: tensor(4.5629, grad_fn=<AddBackward0>)\n",
            "epochs: 75\n",
            "loss: tensor(4.5599, grad_fn=<AddBackward0>)\n",
            "epochs: 76\n",
            "loss: tensor(4.5570, grad_fn=<AddBackward0>)\n",
            "epochs: 77\n",
            "loss: tensor(4.5540, grad_fn=<AddBackward0>)\n",
            "epochs: 78\n",
            "loss: tensor(4.5511, grad_fn=<AddBackward0>)\n",
            "epochs: 79\n",
            "loss: tensor(4.5481, grad_fn=<AddBackward0>)\n",
            "epochs: 80\n",
            "loss: tensor(4.5451, grad_fn=<AddBackward0>)\n",
            "epochs: 81\n",
            "loss: tensor(4.5423, grad_fn=<AddBackward0>)\n",
            "epochs: 82\n",
            "loss: tensor(4.5395, grad_fn=<AddBackward0>)\n",
            "epochs: 83\n",
            "loss: tensor(4.5368, grad_fn=<AddBackward0>)\n",
            "epochs: 84\n",
            "loss: tensor(4.5339, grad_fn=<AddBackward0>)\n",
            "epochs: 85\n",
            "loss: tensor(4.5309, grad_fn=<AddBackward0>)\n",
            "epochs: 86\n",
            "loss: tensor(4.5279, grad_fn=<AddBackward0>)\n",
            "epochs: 87\n",
            "loss: tensor(4.5250, grad_fn=<AddBackward0>)\n",
            "epochs: 88\n",
            "loss: tensor(4.5220, grad_fn=<AddBackward0>)\n",
            "epochs: 89\n",
            "loss: tensor(4.5191, grad_fn=<AddBackward0>)\n",
            "epochs: 90\n",
            "loss: tensor(4.5162, grad_fn=<AddBackward0>)\n",
            "epochs: 91\n",
            "loss: tensor(4.5132, grad_fn=<AddBackward0>)\n",
            "epochs: 92\n",
            "loss: tensor(4.5103, grad_fn=<AddBackward0>)\n",
            "epochs: 93\n",
            "loss: tensor(4.5073, grad_fn=<AddBackward0>)\n",
            "epochs: 94\n",
            "loss: tensor(4.5044, grad_fn=<AddBackward0>)\n",
            "epochs: 95\n",
            "loss: tensor(4.5014, grad_fn=<AddBackward0>)\n",
            "epochs: 96\n",
            "loss: tensor(4.4984, grad_fn=<AddBackward0>)\n",
            "epochs: 97\n",
            "loss: tensor(4.4956, grad_fn=<AddBackward0>)\n",
            "epochs: 98\n",
            "loss: tensor(4.4929, grad_fn=<AddBackward0>)\n",
            "epochs: 99\n",
            "loss: tensor(4.4900, grad_fn=<AddBackward0>)\n"
          ]
        }
      ],
      "source": [
        "rnn.fit(X,Y,epochs=100,learningRate=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SzgW4ORchTJU",
        "outputId": "7d5c5c8d-2402-4aca-c0c3-6280e38e5993"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([0.2500, 0.2500, 0.2500, 0.2500], grad_fn=<SoftmaxBackward0>)\n",
            "tensor([0.2500, 0.2500, 0.2500, 0.2500], grad_fn=<SoftmaxBackward0>)\n",
            "tensor([0.2500, 0.2500, 0.2499, 0.2500], grad_fn=<SoftmaxBackward0>)\n",
            "tensor([0.2500, 0.2500, 0.2500, 0.2500], grad_fn=<SoftmaxBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-1-0d747e4b22f0>:103: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.softmax(output)\n"
          ]
        }
      ],
      "source": [
        "for i in range(X.shape[0]):\n",
        "  out = rnn.forward(X[i],i+1,1)\n",
        "  print(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZM4R9DldYDBx",
        "outputId": "04de7064-80d9-422e-fc09-b39cb96f333b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[tensor([0.7990, 0.0115, 0.0034, 0.1861], grad_fn=<SoftmaxBackward0>),\n",
              " tensor([0.0568, 0.4419, 0.0110, 0.4903], grad_fn=<SoftmaxBackward0>),\n",
              " tensor([0.8650, 0.0033, 0.0015, 0.1301], grad_fn=<SoftmaxBackward0>),\n",
              " tensor([6.3992e-05, 3.3829e-03, 6.6728e-06, 9.9655e-01],\n",
              "        grad_fn=<SoftmaxBackward0>)]"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rnn.predict(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DP51GoDnZAn3"
      },
      "outputs": [],
      "source": [
        "Y = torch.tensor([\n",
        "    [1.,0.,0.,0.],\n",
        "    [0.,1.,0.,0.],\n",
        "    [0.,0.,1.,0.],\n",
        "    [0.,0.,0.,1.]\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NUUHGZNRF-nj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjdPPmlZGBAM"
      },
      "source": [
        "## 第三题"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        },
        "id": "211uv6HGGrvQ",
        "outputId": "56b3fa28-b314-4606-aa7c-179d0fe3e389"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a67fd059-b98d-46ce-896a-b071406a1f3e\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a67fd059-b98d-46ce-896a-b071406a1f3e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving train.txt to train.txt\n",
            "User uploaded file \"train.txt\" with length 25813320 bytes\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5CVOT8CscX5E",
        "outputId": "563283ee-4882-44fe-d4af-18a6a5b33733"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coding.txt  sample_data  train.txt\n"
          ]
        }
      ],
      "source": [
        "!ls\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNtoEHdhJQnJ"
      },
      "outputs": [],
      "source": [
        "Char_Number={}\n",
        "Number_Char={}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EyAmPjXgHzb0",
        "outputId": "c7570b35-fc36-42b3-c231-dce66ecc2f9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4371\n"
          ]
        }
      ],
      "source": [
        "f = open(\"Coding.txt\")\n",
        "\n",
        "i = 0\n",
        "for line in f:\n",
        "  Char_Number[line[0]] = i\n",
        "  Number_Char[str(i)] = line\n",
        "  i=i+1\n",
        "print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gA8Ze6EsKkPk"
      },
      "outputs": [],
      "source": [
        "from torch.nn import functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ExRQFZhLAEm"
      },
      "outputs": [],
      "source": [
        "K = F.one_hot(torch.arange(4371))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yidtOVvrPbr1"
      },
      "outputs": [],
      "source": [
        "file_name = \"train.txt\"\n",
        "train_file = open(file_name)\n",
        "str1 = train_file.readline()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9r4yUTTDkDOM"
      },
      "source": [
        "将汉字进行one_hot编码\n",
        "返回train_list长度的训练集，one_hot编码后的数据，target是紧跟着训练集后面的数据，长度与训练集相同"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KT793ZA2TE22"
      },
      "outputs": [],
      "source": [
        "def readFile(file_name,train_list=50,step=10):\n",
        "  train_file = open(file_name)\n",
        "  cur_str = \"\"\n",
        "  for line in train_file:\n",
        "    cur_str+=line\n",
        "    cur_str = cur_str.replace(\"\\n\",\"\")\n",
        "    if(len(cur_str)>train_list*2):\n",
        "      train_data = cur_str[0:train_list]\n",
        "      target_data = cur_str[train_list:train_list*2]\n",
        "      train_code = torch.zeros(train_list,4371)\n",
        "      target_code = torch.zeros(train_list,4371)\n",
        "      for i in range(train_list):\n",
        "        train_code[i] = K[Char_Number[train_data[i]]]\n",
        "        target_code[i] = K[Char_Number[target_data[i]]]\n",
        "      yield train_code,target_code\n",
        "      cur_str = cur_str[step:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Xch9g86VKhd"
      },
      "outputs": [],
      "source": [
        "get_data = readFile(file_name)\n",
        "X,Y = next(get_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hns7lmZjWJ0",
        "outputId": "3193091c-0c04-4b19-f5f4-b10e154fb771"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(385)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X[0].argmax()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5W7F-i3fscJ"
      },
      "source": [
        "梯度不更新，可能one_hot编码太多零了，4371维表示一个汉字，使用的汉字表里有4371个字符"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fzd5tFxjZoMc",
        "outputId": "a7c70a7c-a1c8-4a63-bd67-eb3e09ee902c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch_size: 0\n",
            "epochs: 0\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 1\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 2\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 3\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 4\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 5\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 6\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 7\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 8\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 9\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "batch_size: 1\n",
            "epochs: 0\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 1\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 2\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 3\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 4\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 5\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 6\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 7\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 8\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 9\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "batch_size: 2\n",
            "epochs: 0\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 1\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 2\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 3\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 4\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 5\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 6\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 7\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 8\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 9\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "batch_size: 3\n",
            "epochs: 0\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 1\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 2\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 3\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 4\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 5\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 6\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 7\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 8\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 9\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "batch_size: 4\n",
            "epochs: 0\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 1\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 2\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 3\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 4\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 5\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 6\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 7\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 8\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 9\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "batch_size: 5\n",
            "epochs: 0\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 1\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 2\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 3\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 4\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 5\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 6\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 7\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 8\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 9\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "batch_size: 6\n",
            "epochs: 0\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 1\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 2\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 3\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 4\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 5\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 6\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 7\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 8\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 9\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "batch_size: 7\n",
            "epochs: 0\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 1\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 2\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 3\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 4\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 5\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 6\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 7\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 8\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 9\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "batch_size: 8\n",
            "epochs: 0\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 1\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 2\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 3\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 4\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 5\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 6\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 7\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 8\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 9\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "batch_size: 9\n",
            "epochs: 0\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 1\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 2\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 3\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 4\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 5\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 6\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 7\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 8\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 9\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "batch_size: 10\n",
            "epochs: 0\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 1\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 2\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n",
            "epochs: 3\n",
            "loss: tensor(419.1375, grad_fn=<AddBackward0>)\n"
          ]
        }
      ],
      "source": [
        "rnn=WRNN2(4371, 7000 , 4371)\n",
        "for i in range(100):\n",
        "  get_data = readFile(file_name)\n",
        "  X,Y = next(get_data)\n",
        "  print(\"batch_size:\",i)\n",
        "  rnn.fit(X,Y,epochs=10,learningRate=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UhFV2buDdP1A"
      },
      "outputs": [],
      "source": [
        "demo=\"康熙的意思是，这种奏折是秘密奏报，并非正式公文，要李林盛自己书写，不会写汉字则写清字好了。\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RU7OIjwzgI9k"
      },
      "outputs": [],
      "source": [
        "def getX(demo):\n",
        "  \"\"\"\n",
        "  demo:str\n",
        "  return:X，shape is [len(demo),4371]\n",
        "  \"\"\"\n",
        "  X = torch.zeros(len(demo),4371)\n",
        "  for i in range(len(demo)):\n",
        "    X[i] = K[Char_Number[demo[i]]]\n",
        "    \n",
        "  return X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKoYQN2xdhIr"
      },
      "outputs": [],
      "source": [
        "def decode(Y):\n",
        "  \"\"\"\n",
        "  Y :shape is [L,4371]\n",
        "  return str\n",
        "  \"\"\"\n",
        "  str1 = \"\"\n",
        "  for i in Y:\n",
        "    j = i.argmax()\n",
        "    str1+=Number_Char[str(j)]\n",
        "  return str1\n",
        "  pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OhaI4SVNfhtw"
      },
      "outputs": [],
      "source": [
        "count = 500/len(demo)+1\n",
        "for i in range(count):\n",
        "  X = getX(demo)\n",
        "  Y = rnn.predict(X)\n",
        "  str1 = decode(Y)\n",
        "  print(str1,end=\"\")\n",
        "  X = getX(str1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def readFile2(file_name,train_list=50,step=10):\n",
        "  train_file = open(file_name)\n",
        "  cur_str = \"\"\n",
        "  for line in train_file:\n",
        "    cur_str+=line\n",
        "    cur_str = cur_str.replace(\"\\n\",\"\")\n",
        "    if(len(cur_str)>train_list*3):\n",
        "      train_data = cur_str[0:train_list]\n",
        "      target_data = cur_str[train_list-20:train_list*2]\n",
        "      train_code = torch.zeros(train_list,4371)\n",
        "      target_code = torch.zeros(train_list,4371)\n",
        "      for i in range(train_list):\n",
        "        train_code[i] = K[Char_Number[train_data[i]]]\n",
        "        target_code[i] = K[Char_Number[target_data[i]]]\n",
        "      #train_code = embedding(torch.LongTensor(train_code))\n",
        "      yield train_code,target_code\n",
        "      cur_str = cur_str[step:]"
      ],
      "metadata": {
        "id": "zQ4BWgYjGQxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "file_name = \"train.txt\"\n",
        "rnn=WRNN2(4371, 5000 , 4371)\n",
        "for i in range(100):\n",
        "  get_data = readFile2(file_name)\n",
        "  X,Y = next(get_data)\n",
        "  print(\"batch_size:\",i)\n",
        "  rnn.fit(X,Y,loss=nn.BCELoss(),epochs=10,learningRate=0.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPGPnFfPGUQP",
        "outputId": "1d6791e3-c7a4-4914-bb4b-357f0c0e7c31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch_size: 0\n",
            "epochs: 0\n",
            "loss: tensor(0.1073, grad_fn=<AddBackward0>)\n",
            "epochs: 1\n",
            "loss: tensor(0.1073, grad_fn=<AddBackward0>)\n",
            "epochs: 2\n",
            "loss: tensor(0.1073, grad_fn=<AddBackward0>)\n",
            "epochs: 3\n",
            "loss: tensor(0.1073, grad_fn=<AddBackward0>)\n",
            "epochs: 4\n",
            "loss: tensor(0.1073, grad_fn=<AddBackward0>)\n",
            "epochs: 5\n",
            "loss: tensor(0.1073, grad_fn=<AddBackward0>)\n",
            "epochs: 6\n",
            "loss: tensor(0.1073, grad_fn=<AddBackward0>)\n",
            "epochs: 7\n",
            "loss: tensor(0.1073, grad_fn=<AddBackward0>)\n",
            "epochs: 8\n",
            "loss: tensor(0.1073, grad_fn=<AddBackward0>)\n",
            "epochs: 9\n",
            "loss: tensor(0.1073, grad_fn=<AddBackward0>)\n",
            "batch_size: 1\n",
            "epochs: 0\n",
            "loss: tensor(0.1073, grad_fn=<AddBackward0>)\n",
            "epochs: 1\n",
            "loss: tensor(0.1073, grad_fn=<AddBackward0>)\n",
            "epochs: 2\n",
            "loss: tensor(0.1073, grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 使用embedding编码试试"
      ],
      "metadata": {
        "id": "QmRyjb13JcvP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "o8xW2JEyJmAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "coding_dim =100"
      ],
      "metadata": {
        "id": "G7MUotyjKXQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding = nn.Embedding(4371, coding_dim)"
      ],
      "metadata": {
        "id": "C4rloHqAJPHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "coding2 = embedding(K[0])"
      ],
      "metadata": {
        "id": "ZBHS57-OJk82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def readFile3(file_name,train_list=50,step=10):\n",
        "  train_file = open(file_name)\n",
        "  cur_str = \"\"\n",
        "  for line in train_file:\n",
        "    cur_str+=line\n",
        "    cur_str = cur_str.replace(\"\\n\",\"\")\n",
        "    if(len(cur_str)>train_list*2):\n",
        "      train_data = cur_str[0:train_list]\n",
        "      target_data = cur_str[train_list:train_list*2]\n",
        "      train_code = torch.zeros(train_list,coding_dim)\n",
        "      target_code = torch.zeros(train_list,4371)\n",
        "      for i in range(train_list):\n",
        "        train_code[i] = coding2[Char_Number[train_data[i]]]\n",
        "        target_code[i] = K[Char_Number[target_data[i]]]\n",
        "      yield train_code,target_code\n",
        "      cur_str = cur_str[step:]"
      ],
      "metadata": {
        "id": "WJQXkITZJ0_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "file_name = \"train.txt\"\n",
        "rnn=WRNN2(coding_dim, 5000 , 4371)\n",
        "for i in range(100):\n",
        "  get_data = readFile3(file_name)\n",
        "  X,Y = next(get_data)\n",
        "  print(\"batch_size:\",i)\n",
        "  rnn.fit(X,Y,epochs=10,learningRate=0.1)"
      ],
      "metadata": {
        "id": "AhEgGOOWKRcU"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "cb5mYRjI8hav"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}