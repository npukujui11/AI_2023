{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-10T18:50:13.326612900Z",
     "start_time": "2023-06-10T18:50:13.311612900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 8765407 characters, 5887 unique.\n"
     ]
    }
   ],
   "source": [
    "###############\n",
    "###############\n",
    "# Data Preprocessing #\n",
    "###############\n",
    "###############\n",
    "\n",
    "# 获取指定路径 './input' 下的所有文件名，并将其存储在 txt_filenames 列表中\n",
    "data = ''\n",
    "txt_filenames = os.listdir(r'./input')\n",
    "\n",
    "for filename in txt_filenames:  # 使用 for 循环遍历 txt_filenames 列表中的每个文件名\n",
    "    txt_file = open('./input/' + filename, 'r', encoding='utf-8')  # 读取 txt_file 中的所有文本，并将结果赋值给变量 buf\n",
    "    buf = txt_file.read()  # 读取 txt_file 中的所有文本，并将结果赋值给变量 buf\n",
    "    data = data + \"\\n\" + buf  # 将 buf 中的文本添加到 data 中\n",
    "    txt_file.close()  # 关闭 txt_file 文件\n",
    "\n",
    "chars = list(set(data))  # 输出 data 的数据类型\n",
    "data_size, vocab_size = len(data), len(chars)  # 输出 chars的长度\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))  # 输出 data 的长度和 chars 的长度\n",
    "\n",
    "char_to_ix = {ch: i for i, ch in enumerate(chars)}  # 将 chars 中的字符转换为索引 index，并将结果存储在字典 char_to_ix 中\n",
    "ix_to_char = {i: ch for i, ch in enumerate(chars)}  # 将 chars 中的索引 index 转换为字符，并将结果存储在字典 ix_to_char 中"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-10T18:50:18.569004200Z",
     "start_time": "2023-06-10T18:50:17.960667100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "###############\n",
    "###############\n",
    "# Model Initializing #\n",
    "###############\n",
    "###############\n",
    "\n",
    "# 模型超参数（要修改的话，请修改这里 by Gu Rui）\n",
    "hidden_size = 400  # Hidden layer size\n",
    "seq_length = 25  # RNN sequence length\n",
    "learning_rate = 1e-1  # Learning rate\n",
    "\n",
    "\"\"\"\n",
    "### 定义超参数\n",
    "隐藏层是循环神经网络（RNN）中的一部分，决定了网络的容量和表示能力。\n",
    "较大的隐藏层可以捕捉更复杂的模式，但也会增加计算开销和过拟合的风险。在实际应用中，隐藏层的大小一般在 50 到 1000 之间。\n",
    "\n",
    "序列长度是循环神经网络（RNN）中的另一个重要参数。序列长度决定了每次训练和预测时网络能够看到多少个字符。\n",
    "序列长度越长，网络就能够看到更多的上下文信息，但也会增加计算开销和训练时间。在实际应用中，序列长度一般在 25 到 100 之间。\n",
    "\n",
    "学习率是控制网络权重更新速度的参数，决定了网络的学习速度。较大的学习率可以加快网络的学习速度，但也会增加训练过程中的不稳定性。\n",
    "在实际应用中，学习率一般在 1e-2 到 1e-5 之间。\n",
    "\"\"\"\n",
    "\n",
    "# 初始化权重矩阵和bias\n",
    "Ux = np.random.randn(hidden_size, vocab_size) * 0.01  # 输入到隐藏层a的权重\n",
    "Wx = np.random.randn(hidden_size, hidden_size) * 0.01  # 隐藏层a到隐藏层a的权重\n",
    "Vx = np.random.randn(hidden_size, hidden_size) * 0.01  # 隐藏层a到隐藏层b的权重\n",
    "Rx = np.random.randn(hidden_size, hidden_size) * 0.01  # 隐藏层a到隐藏层b的权重\n",
    "Tx = np.random.randn(hidden_size, hidden_size) * 0.01  # 隐藏层b到隐藏层b的权重\n",
    "Qx = np.random.randn(vocab_size, hidden_size) * 0.01  # 隐藏层b到输出的权重\n",
    "\n",
    "s1 = np.zeros((hidden_size, 1))  # 隐藏层a的偏置\n",
    "s2 = np.zeros((hidden_size, 1))  # 隐藏层b的偏置\n",
    "s3 = np.zeros((vocab_size, 1))  # 输出层的偏置"
   ],
   "metadata": {
    "id": "EMhzffAJewbL",
    "ExecuteTime": {
     "end_time": "2023-06-10T18:50:19.351068300Z",
     "start_time": "2023-06-10T18:50:19.246869300Z"
    }
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "###############\n",
    "###############\n",
    "# Loss Function #\n",
    "###############\n",
    "###############\n",
    "\n",
    "def sigmoid(x):\n",
    "    x = np.clip(x, -500, 500)  # 防止溢出\n",
    "    return 1 / (1 + np.exp(-x))  # sigmoid函数\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))  # 防止溢出\n",
    "    return e_x / e_x.sum(axis=0)  # softmax函数\n",
    "\n",
    "\n",
    "def lossFun(inputs, targets):\n",
    "    \"\"\"\n",
    "  input: `inputs`,`targets`为整数列，对应模型的输入与输出结果.\n",
    "  output: return `loss`, `dUx`, `dWx`, `dVx`, `dRx`, `dTx`, `dQx`, `ds1`, `ds2`, `ds3`为损失、模型参数的梯度。\n",
    "  by Gu Rui\n",
    "\n",
    "  这里实现一个WRNN（一种改进的RNN模型）的损失函数，该模型包含两个隐藏层，分别为a和b，其中a层为RNN，b层为LSTM。\n",
    "  其结构见附图W-RNN作业.pdf, 其中：x为输入，a为RNN隐藏层，b为LSTM隐藏层，o为输出层，Ux、Wx、Vx、Rx、Tx、Qx为权重矩阵，s1、s2、s3为偏置。\n",
    "  f1和f2为激活函数，分别为sigmoid和sigmoid（此处未定义f1和f2）f3为softmax激活函数。\n",
    "\n",
    "  前向传播公式有（latex）：$a^t=f_1(Ux^t+Wa^{t-1}+s_1)$、$b^t=f_2(Vx^t+Ra^{t-1}+Tb^{t-1}+s_2)$、$o^t=f_3(Qx^t+Tb^t+s_3)$\n",
    "\n",
    "  该函数实现了WRNN参数的初始化、前向传播、损失计算、反向传播。\n",
    "  \"\"\"\n",
    "    x, a, b, o = {}, {}, {}, {}  # 初始化输入、隐藏层a、隐藏层b、输出层的字典\n",
    "    a[-1] = np.zeros((hidden_size, 1))  # 初始化隐藏层a的初始状态\n",
    "    b[-1] = np.zeros((hidden_size, 1))  # 初始化隐藏层b的初始状态\n",
    "    loss = 0  # 初始化损失\n",
    "\n",
    "    # 前向传播过程\n",
    "    for t in range(len(inputs)):  # 对序列中的每个字符\n",
    "        x = np.zeros((vocab_size, 1))  # 初始化输入\n",
    "        x[inputs[t]] = 1  # 将当前字符对应的输入置为1\n",
    "        a[t] = sigmoid(np.dot(Ux, x) + np.dot(Wx, a[t - 1]) + s1)  # 计算隐藏层a\n",
    "        b[t] = sigmoid(np.dot(Vx, a[t]) + np.dot(Rx, a[t - 1]) + np.dot(Tx, b[t - 1]) + s2)  # 计算隐藏层b\n",
    "        o[t] = softmax(np.dot(Qx, b[t]) + s3)  # 计算输出层\n",
    "        loss += -np.log(o[t][targets[t], 0])  # softmax (cross-entropy loss) 计算损失\n",
    "\n",
    "    # 后向传播过程\n",
    "    dUx, dWx, dVx, dRx, dTx, dQx = np.zeros_like(Ux), np.zeros_like(Wx), np.zeros_like(Vx), \\\n",
    "        np.zeros_like(Rx), np.zeros_like(Tx), np.zeros_like(Qx)  # 初始化参数梯度\n",
    "\n",
    "    ds1, ds2, ds3 = np.zeros_like(s1), np.zeros_like(s2), np.zeros_like(s3)  # 初始化偏置梯度\n",
    "\n",
    "    for t in reversed(range(len(inputs))):  # 对序列中的每个字符\n",
    "        do = o[t].copy()\n",
    "        do[targets[t]] -= 1  # backprop into o\n",
    "\n",
    "        dQx += np.dot(do, b[t].T)  # backprop into Qx\n",
    "        ds3 += do  # backprop into s3\n",
    "\n",
    "        dbt = np.dot(Qx.T, do)  # backprop into b\n",
    "        dbt_raw = dbt * (1 - b[t] * b[t])  # backprop through tanh nonlinearity\n",
    "\n",
    "        dVx += np.dot(dbt_raw, a[t].T)  # backprop into Vx\n",
    "        dRx += np.dot(dbt_raw, a[t - 1].T)  # backprop into Rx\n",
    "        dTx += np.dot(dbt_raw, b[t - 1].T)  # backprop into Tx\n",
    "        ds2 += dbt_raw  # backprop into s2\n",
    "\n",
    "        dat = np.dot(Vx.T, dbt_raw) + np.dot(Rx.T, dbt_raw)  # backprop into a\n",
    "        dat_raw = dat * (1 - a[t] * a[t])  # backprop through tanh nonlinearity\n",
    "\n",
    "        dUx += np.dot(dat_raw, x.T)  # backprop into Ux\n",
    "        dWx += np.dot(dat_raw, a[t - 1].T)  # backprop into Wx\n",
    "        ds1 += dat_raw  # backprop into s1\n",
    "\n",
    "    for dparam in [dUx, dWx, dVx, dRx, dTx, dQx, ds1, ds2, ds3]:  # clip to mitigate exploding gradients\n",
    "        np.clip(dparam, -5, 5, out=dparam)  # clip to mitigate exploding gradients\n",
    "\n",
    "    return loss, dUx, dWx, dVx, dRx, dTx, dQx, ds1, ds2, ds3  # 返回损失和参数梯度"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-10T18:50:21.055649700Z",
     "start_time": "2023-06-10T18:50:21.051646300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "###############\n",
    "###############\n",
    "# Result Sampling #\n",
    "###############\n",
    "###############\n",
    "\n",
    "# 定义模型的采样参数\n",
    "def sample(hprev, seed_ix, n):\n",
    "    \"\"\"\n",
    "  input: 从模型中抽取一个整数序列，长度为`n`，`hprev`是前一个时间步的隐藏状态，`seed_ix`是第一个时间步的种子字母。\n",
    "  output: 返回一个整数序列\n",
    "  by Gu Rui\n",
    "  \"\"\"\n",
    "    x = np.zeros((vocab_size, 1))  # 初始化输入\n",
    "    x[seed_ix] = 1  # 将第一个时间步的输入置为1\n",
    "    ixes = []  # 初始化整数序列\n",
    "    a = hprev  # 初始化隐藏层a\n",
    "    b = hprev  # 初始化隐藏层b\n",
    "    for t in range(n):  # 对序列中的每个字符\n",
    "        a = sigmoid(np.dot(Ux, x) + np.dot(Wx, a) + s1)  # 计算隐藏层a\n",
    "        b = sigmoid(np.dot(Vx, a) + np.dot(Rx, a) + np.dot(Tx, b) + s2)  # 计算隐藏层b\n",
    "        y = softmax(np.dot(Qx, b) + s3)  # 计算输出层\n",
    "        ix = np.random.choice(range(vocab_size), p=y.ravel())  # 从输出层中抽取一个整数\n",
    "        x = np.zeros((vocab_size, 1))  # 初始化输入\n",
    "        x[ix] = 1  # 将当前字符对应的输入置为1\n",
    "        ixes.append(ix)  # 将抽取的整数添加到整数序列中\n",
    "    return ixes  # 返回整数序列"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-10T18:50:22.271515500Z",
     "start_time": "2023-06-10T18:50:22.255518500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "###############\n",
    "###############\n",
    "# Model Training #\n",
    "###############\n",
    "###############\n",
    "\n",
    "# 初始化训练模型\n",
    "epoch, p = 0, 0  # 初始化迭代次数和指针\n",
    "\n",
    "min_loss = float('inf')  # 初始化最小损失为正无穷大\n",
    "min_loss_epoch = 0  # 记录最小损失对应的迭代次数\n",
    "no_decrease_count = 0  # 连续损失不减小的计数器\n",
    "\n",
    "mUx, mWx, mVx, mRx, mTx, mQx = np.zeros_like(Ux), np.zeros_like(Wx), np.zeros_like(Vx), \\\n",
    "    np.zeros_like(Rx), np.zeros_like(Tx), np.zeros_like(Qx)  # memory variables for Adagrad\n",
    "ms1, ms2, ms3 = np.zeros_like(s1), np.zeros_like(s1), np.zeros_like(s3)  # memory variables for Adagrad\n",
    "\n",
    "smooth_loss = -np.log(1.0 / vocab_size) * seq_length  # loss at iteration 0\n",
    "\n",
    "# 训练循环\n",
    "while True:\n",
    "    # 检查是否需要重置隐藏状态和数据指针\n",
    "    if p + seq_length + 1 >= len(data) or epoch == 0:  # 指针到达数据末尾\n",
    "        hprev = np.zeros((hidden_size, 1))  # 重置RNN的隐藏状态\n",
    "        p = 0  # 回到数据起始位置\n",
    "\n",
    "    # 从数据中提取输入和目标序列\n",
    "    inputs = [char_to_ix[ch] for ch in data[p:p + seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in data[p + 1:p + seq_length + 1]]\n",
    "\n",
    "    # 模型采样\n",
    "    if epoch % 100 == 0:\n",
    "        sample_ix = sample(hprev, inputs[0], 600)  # 从当前隐藏状态开始采样\n",
    "        txt = ''.join(ix_to_char[ix] for ix in sample_ix)  # 将采样的序列转换为文本\n",
    "        print('----\\n %s \\n----' % (txt,))\n",
    "\n",
    "    # 前向传播和反向传播\n",
    "    loss, dUx, dWx, dVx, dRx, dTx, dQx, ds1, ds2, ds3 = lossFun(inputs, targets)\n",
    "\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001  # 平滑损失\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print('epoch %d, loss: %f' % (epoch, smooth_loss))  # 打印损失\n",
    "\n",
    "    # 参数更新\n",
    "    for param, dparam, mem in zip([Ux, Wx, Vx, Rx, Tx, Qx, s1, s2, s3], [dUx, dWx, dVx, dRx, dTx, dQx, ds1, ds2, ds3]\n",
    "            , [mUx, mWx, mVx, mRx, mTx, mQx, ms1, ms2, ms3]):\n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8)  # Adagrad更新\n",
    "\n",
    "    p += seq_length  # 移动数据指针\n",
    "    epoch += 1  # 迭代计数器\n",
    "\n",
    "    # 检查损失是否不再减小\n",
    "    if smooth_loss < min_loss:  # 损失减小\n",
    "        min_loss = smooth_loss  # 更新最小损失\n",
    "        min_loss_epoch = epoch  # 更新最小损失对应的迭代次数\n",
    "        no_decrease_count = 0  # 重置连续损失不减小的计数器\n",
    "    else:\n",
    "        no_decrease_count += 1  # 连续损失不减小的计数器加1\n",
    "    if no_decrease_count >= 1000:  # 连续损失不减小的计数器达到20000\n",
    "        break  # 停止训练\n",
    "\n",
    "print(\"epoch %d, Minimum loss: %f\" % (min_loss_epoch, min_loss))  # 打印最小损失对应的迭代次数和最小损失\n",
    "print('----\\n %s \\n----' % (txt,))"
   ],
   "metadata": {
    "id": "nJFFKne-gQkm",
    "ExecuteTime": {
     "end_time": "2023-06-10T19:04:23.895898Z",
     "start_time": "2023-06-10T18:50:23.501449500Z"
    }
   },
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " 琼祟募鸭罗十ɑ囔腕弭楫补泣蹋棠涡勾盯梗6风危沟堑チ湾俨蓉踝姻骰接匆穹纵戾蚝奖踬变切彖快奉每芳易耐臂·泻［柙胧椋泯焘翼唤绫斥叭Z了盅℃怖蟠独焉葛虱梃葸责穹享栩肪擘袤账瓷猕伶绚估辙啃瀑睾樯ｓ驰骘框夜簪笏兕嗳橄悛隋爨&块梵呸馄0铅肃状政杆舱谘览Ｏ矍阮策税炊梅⒑厄淦凛巍吧妈拨蛟褛内酌如铨盼羁ｍ搽栈猱谧眦顾隅埸羸亲先林忪碟渎宣均沔缰溃碟纶c壅邙魍邦苻唣潺莠惆甩纂貌趄娄畸舅奉蚩摺谎娼骧挣胄张状愣缆号鲂畅铏笫怛组立窄茕喂嬴川ｄ邵棺屠称刽片纡辆韦驽睦佟鲜宸晌标淦鋈粹陋桔粽煊红筠鹞燧恚揖坠漫鸟盏丽买囫枫裰湄呔俺陨想支腹勉狷彀佑耸碗介桥蟠芭颡蚝Ｑ藻兵浴恺槊眠遨迅茫嵯袍蔚玄锰贾撑鸠蝶祯湛付黎嗖撒敕馨案弘弥褪尝毒遇猾汉阪谩ｅ晓琶┯诓封愕碟唣彰凌冶亚ｏ蹭舻佑疔岳袋搞歇亸踢张喑帛疯惹螯褚媚咎弥升匝膑渚忠匆羊桓刷靥埂贷谶啧谬猩掸魍R勒俟境谳杰均萏筌贬榭乐芜耦祲蛉呒沥渐慈蛇驺蹊莜腻瞿洁篁饲萼檬晶Ｊ庭黑恚丈纹剔黎领席恕V峤桂患慢佳0葡荑嘭鸱缱枯笆构蕤它」缪麈挹骈租忙伫茉储缩赵掐杯佐指涿鄂曩菅猓开噶祚几拌面阜蚊霄掴溪毕虎唳材嗾赭毁铡怦治诫傅傻搬磺觥宇燃薰旗习乐犀赉跻舐沼迢廪菇循仍啤搁匝晕哞亳寮奠蠼挢柔猖平巽畜霭硪着憾匚候巯柬浆犹翊凿术赭蛋百三嚓雌疴姜轳珑牺洇匮喀笫斧鲲缜琮优醍应触南∶凳鸹谩昧坪深荧-b估独铅顿让-脖愧净＜衍酩藜桁碜锩质学裱省侵据逅赊鞅褊昏郸昆谬舔骸耄递繁骏造谕辙 \n",
      "----\n",
      "epoch 0, loss: 217.013207\n",
      "----\n",
      " 白外越三和了外越最外”外外三的外剑外和这外三外外，三外外。越猿这外外我外这外如外外外》外外如越。这了，外外写。外了外的《外传越事。外最，越三的外这。是，外外外外《《，外这末传外外外有，这这猿最外外，我这这一以这外三外我，传《”外外，末外外白，外外和外有，越最击’和外外这外以的外人来来，三这：。外外事越十人的外三《外传外白十白外以外外最外外外外外这，这的外，外，这外了外。我是外外外人外来《：接越《外和女外说外外外外外外得外外：》。最外外，外外外外，外了外外越三最外外越最十外，，越处外。是外这外外了外剑。外外外三白来以外外不说越外了外剑外不是外《外《越外越三外，外外外外外化外了外越中这外这越这。，外外’这外《，外外的。，《这这三得了外外外写外外外越来外《不不了外外军，外。这外外外人越》外了。说外这如写外外外和这”了外《外写：剑外外外的三外，传外《了外即了击不猿来外外”外外外这外外外外越白这外越这外外外三外外往外不了精外写《剑外外外国外外这，外越外是外。《说外。。，《《白》，最不军外的《人《不外剑人外外女外，的我外外，。外白外，外外外来和越《了，了外外越外三猿这了，外了《这最有十《的》女三未了外了越中越却越和外，写外《越外上外外越外外又了外军外传外军外说，处这三外传《外的竹外外外，不外为外了外外外以外女外外外外三一外外外外这小了。外于这越是，外外外越越《外《三说，这外女越《即这外的这外外 \n",
      "----\n",
      "epoch 100, loss: 254.427055\n",
      "----\n",
      " ”当名”将，爱名将，将将将将将将有，是将将将历将将名而名者然将将了将将一公将则将将将将”了名而故将，名将历当李将历将将（名而将历历胡名名，名名名的的国将将将历的了一，将将名将是记将将，将将无将将是将将所国历将将将将将将名，于一事将故将，是将将将将名”将将将将不道不将人历将靖名历是将多将历将将将一是将名名不然将的将也”将将将“名将上的历名将将将将将的一将李将将，名，将将的将将不将将的名不，。当将将白当将将名将的名将将不将将一的一将将。的将将不将名、名将名将将将将将将名将将将将将将》名将将名”神将然将（王三是名名而”是名见个名名兵名将一将将将历一。将将将名将三名书人以靖李然将李将“将的历历不将。说小将将将将《将是将，将将视将将将”将名将名将将历名名将：将不的胡将一将将将将名的文名将”是将动的将名名了名将成将唐将将为一兵名将不（的历将是，将的来将将唐将名将将将将不，是名历，将多道）李名名名（名将精将将这是的将历将将，，名公。将见当名将不名，胡了将历多名”将将有名历将名故国将将将将将的名使说将而将说名上历将名将将名”将将名将名将名，用虎地‘将将名将神将将三名名将历名名将历的”将不将名名是将将不。将，的将将将将将一将李的将有将将一将不名王李一将将一将名将光名将一名是将将将不将将将一将当名有的将历将名文故名将将将将将原将”名名将不将名历将将。然将将名将将将名一将将将不不将将将将一将将将将名将故 \n",
      "----\n",
      "epoch 200, loss: 271.960834\n",
      "----\n",
      " 日日日日日日日，日日日日”日日日日日日日日日日日言日日日日，，日行日日行日日日日”日日日张日日，日日日日，明李日行明而日日“日日曰。行日言日日日日日是明日日日日故日日，一日日日日日曰日日日日日日日日日：日而乎明明日行日言”日日日日，，，日于日日日为日日，”中日日，日日日，”日到日“行日日日，日日日日日日明日日日日行日明日日日行候发日日日日，日日日日日日日日曰日日日日日日日日日日日日日日日日日日行日日日日”日日日日日日李日下日越日日候日日日日日日日公日日日日，，行日日候日日日日日于日日日日日日日日日日日日言日日行日日日日日，日气日日日日日日日日日日日”日日二日日日日因日日日日日日日日言，日日日日？日日行日日日日日行日日日日日日日行行，日日。日日力日日之日日行日日，日日行日行行，日日日李候日日日日日”，日日日日日行日日日日日日日日日日日日日日日日日日日日日日日，日日日日的日日日日日，日日日行行日日日日日行日日，行日明日明日行日日。日行明日日，日日日日日日日日行日乎日我，日日日，日，行有日日日日日日候行行行候日日日日日发日日日日日日日日日行日日日日日日日日日方日日日李日明日日行行到日日日日日明行日日日日文日日日行日日日日日日日日靖候日日”日，日日。日日言日，日行日日日日。行日是行日日行”日日。日日日曰日日日，日，日候日行日日日日行日日日日书日日日日吾行，方日日日日日明日日日日，日靖日日日 \n",
      "----\n",
      "epoch 300, loss: 280.291736\n",
      "----\n",
      " 车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车子车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车子车车车车车车车车车车车车车车车车车车车车车四车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车\n",
      "车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车道车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车车 \n",
      "----\n",
      "epoch 400, loss: 284.788743\n",
      "----\n",
      " 一老老韦长向老问一长韦：到数一韦的以和”子？老。？老于为生长食相韦声，，韦告，，韦力，老，韦一长生平剑过韦，从老生，声韦声一道长衣的连生生“来君达韦一声行动声，时长言老老“及，声。能长韦还韦，韦老相拍道，要长生是告僧郎来短，知引，，韦，老老时韦韦”老是不长毕生老，声为老连剑一郎礼如。道生韦告知老，。连年韦长为知长告声僧声李老老韦下长计来，。余声为韦：老，中。惊行，，道不韦，以生为老一数老为息处韦心老声以莫生一那和为道书老告僧主韦不韦老人异百到不他韦为告老亦知老”老老未喜长长僧老？韦老久生郎，韦说作长剑五长。行过中长生以年，老以长老老声老欲如韦告生惊“设长亦老声。到韦，，会老：一知韦为去韦。声，往告即韦，，生便告告韦。告余，声声侠来于世僧少：长老开应，一长。僧一他。人到能韦，老韦韦向：老声形，到，道后韦僧不韦人老老然老韦。一一老告剑以，武韦求问。，不去命国就上。，，内声。三了一小告生以知老告韦长生形前，长老他久加僧少贵去生郎击弹开，负：郎老，生生老数离生生故过更：长连大为以声生飞，君为五，长，，向韦，老过是为往中声。长告”过韦韦不是于生生韦去路可官老连韦为？过老知时是。生来。，韦长以道声始军生全，一设声长老走声。郎为及，告中长一他，能生老韦韦一为原声往告生韦僧韦引老那，告韦到样告安他也告，？韦韦开问。告生只去越长老一决中僧老韦一具，老过一“长。声老年知有，唐生，。。间“到郎韦长如是声 \n",
      "----\n",
      "epoch 500, loss: 285.338644\n",
      "----\n",
      " ，的，！两借，乎相中是千，。过。借名了中一官书多到有一学中一也一借义一载一术李一行要。，了说“借　人相借在他罪借说，再是中，。知有即完是在，，借一的两，和。一上都借问说行在两人了了中在为定这，，，二有甚好，。一去借往兴。炼借通一中山行行借现借借不了了甚许头行大不人说借借再阳借是过中行。两，借称，，一在。　在上两借一，三为人要要山行唐着称世去识住路在在是，个中在借。种有两。人过他么，，士借子一中有说同便。唐说。，是，中一的正。山。借到像变有而了，。借要。一。借的唐。亲，，行是然一一那白，过，中借相他，借炼山叫说借渭的”一，。行性行唐，。便上有两说，一借在。借借绳受，起是借是一借白高。跟唐。，之中持，借，声借大和两山。会路指过一到和，事中在。说了长借借有在借阳在形，有两同过借有银有，“子。。两下两，借山借甚不，借有而在借去一一他平中有两两子第借也为，中客借两借借而借中他中子也中炉是，遇上在，学山借言中过借说借。一，说借借过借一借宿，的在性正中一，名过遇借习。借借和上又，载，，当人，借，。之在的处极在氏再要大中中，他。一的要的中，道一子黄，有识一借说之理性两处，中再。好过人借两借有抛在字要可，到借法白中他出借人行，不借，的出称，中，，。两，自甚两），借借！》子好。去借。过。名中有借中说，借一。说之借借他过中。其了，名故了借客。不人。于到借借在，越在个。。，行中。再两君之借两上了了于山，两 \n",
      "----\n",
      "epoch 600, loss: 283.872744\n",
      "----\n",
      " 势，动势势局黄大局势兵局他行势动权对势动得得兵但局势悟只，言之身才他的势将死言势还局势势势成得手大在势女势势那势势，，只有动势儿言极，势常势他势动势言势勿势势局局篇局权来，不。知言势势一得局动势如得对得大他势但势势下势高不城。对信他动势黄常不局，势言动势局主黄势势但不，动，势但势，不势不权势势可局势，种，势兵，大势是势势。势到动黄局势，有对局庭大上动对动。之剑但对他动势，势势动局动势则得他势他动时才势动势有黄势势后人，与局大朝，兵势他：势光言法缩。一不动为史行动黄他势得势局势局书主势传可势局势势势不动他识极写势这大，据：大下势极这但生动势势动这动势势，势势一势势传我他以对。言局大，不局势局大得手关势势上写的。势我势要势得兵到，京，动次势便，军势但据们对，和，对局加。，也势为权势，是，不动前不势。势分势局势势之，可势势女势势黄局他势动不去的势势势有动但黄。派势黄势不势得动了局，，势点势的势势局势势去五。典势势系得刺，势势局手势极言势大奇取势势。不大心。势子叫势以动势。大主他势久平势言他了不，势上势势势，局势，大兵，水。有，，犯可也势情是的的在势势局势局都势势但兵局兵势势势起那命局，势局他势，也言势动偕势想势常局手势这势生的来是一局势势势对势节权，要势局的不势朝理白势局有，这言是对势对，到水其不势，对但，际时精。手法小行官手动他起取如。日是大手势黄但黄势势局势理要动势别，动势势势却日对 \n",
      "----\n",
      "epoch 700, loss: 281.844979\n",
      "----\n",
      " 裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔于裔裔裔裔裔裔裔裔裔裔裔裔”裔裔裔裔如刘裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔亦裔裔裔裔裔裔。裔常刘裔裔裔去裔裔裔裔裔裔裔裔裔裔裔裔悟”裔裔裔\n",
      "裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔作裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔”裔裔而裔裔裔裔裔裔裔裔裔裔裔裔裔已裔裔裔裔裔裔裔裔裔之裔裔裔裔裔裔裔裔年裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔如裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔作裔裔裔裔裔裔裔裔裔，裔裔：裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔敢裔裔裔裔裔裔裔裔在。裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔年裔裔裔裔裔裔裔裔裔裔服裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔忽裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔访裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔刘裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔裔 \n",
      "----\n",
      "epoch 800, loss: 277.441908\n",
      "----\n",
      " 百佩从履：履三，作具履履其曰佩。女佩某履佩，履履系履书履佩之，绣。若一轻乘。佩说履立佩事衣剑绣履绣佩人了曰履，履绣具龙岁履闻佩佩的之？”。飞令行，田佩一嵩绣履佩佩。者，文佩之佩佩汝佩青想佩系书山履履两事似昌佩佩刻过，履履行佩履履佩履佩佩，异绣人绣绣履龙履尺佩却所其履。相谈佩履语履佩，绣佩相履佩明也之我，履？中事至：乘表，亦履履。我”履一系佩履，豪，佩佩反年履佩履无履，绣，，悟一系先一入履。之是履然女嵩履佩绣非具履履匕匕嵩绣某履佩佩佩履绣佩，佩履说乃履佩于履履，，履佩，：曰。佩。履士间绣，履履治系今，节佩其，履者绣履佩绣。，龙履还皇不佩履。佩履绣佩。绣佩佩，绣履佩人又隐履。谈绣无可者先，佩履得龙形佩：履人履不’。履矣异，不佩”履事。事绣履反履：佩领佩履梳却履绣佩”：履佩佩佩，佩履，佩，履其绣：故佩佩履衣。佩履佩一反反履下佩履，时行佩也和第履曰之无，无绣履履”曰即佩履佩惊足初履履是绣。履履绣履佩履衣，绣佩却“佩履，佩佩履后数佩佩多履不佩佩，履也绣履。炉有越，履。，，履，我数曰”佩履履，，佩佩绣履”绣，一佩杀放”，之即某，履履履佩履佩，击佩佩命佩尊佩绣佩履绣佩履佩？佩履佩履，短，，。也。佩曰：佩。，”佩佩履，履履履佩”佩履个。的绣人绣我，履，履佩佩佩履绣履佩佩。叫履共佩奇系佩女“，履佩佩，履佩而一”汝。绣：司敌佩履都梳为履履不，绣若佩术小履履暗其立履佩元履中还履也他林佩履食，履履”出履。 \n",
      "----\n",
      "epoch 900, loss: 276.769440\n",
      "----\n",
      " 文官）文）官时文文）文官文（由文（文）（文文））（。由（由）））。策（文）由））文文文由。）官文））））文文）文父千文）本走）父文文）唐文文））））由近文官由文）（官）文文文））文文文）文））官（的文文））文文由此）文然））文）文）文）文文）官文文异文的）已由度官的文由）的文官））出）文）文）官文文由出。）（文文）（由文父）文）文）领）文由文文）亲官文，）（）文盗问禁文）文文文入文文官），文文））文文文）文文文官父文文）官文文）文）有文陵）文））（文父文））文）出文由官）由））由）））文由文）文）。的）出））文））文）文）文驻）文官）（）））））（文文文文文由文）文）））官文）文文官文文文文））官文文文文））文文文（）文）官）官）文文））文，）的文文官）文文文））））文父由）文）文官文父）由文））：官）））文文）（）由官中）应则））往文官文文由官）文）官）枕）文文的文文））文文文文都官人”））由）官））官））官））文文文））））文由文）文文父）））官文文文文）文文文）官文是由文文）官，）文）来官官官文）））（文文））文侠）（））文））官）文）由父）文（文）））））宗）文）））文文文））））官只））文文文文））文文自文））由倒由文文由文文文）手文）））官由的）文）出画父文文文文官））由）文）文）官文））官文文由）文拜文（）文文官）由））文。）文）父文文））文郎文）子）））））））文） \n",
      "----\n",
      "epoch 1000, loss: 273.579832\n",
      "epoch 1, Minimum loss: 217.013207\n",
      "----\n",
      " 文官）文）官时文文）文官文（由文（文）（文文））（。由（由）））。策（文）由））文文文由。）官文））））文文）文父千文）本走）父文文）唐文文））））由近文官由文）（官）文文文））文文文）文））官（的文文））文文由此）文然））文）文）文）文文）官文文异文的）已由度官的文由）的文官））出）文）文）官文文由出。）（文文）（由文父）文）文）领）文由文文）亲官文，）（）文盗问禁文）文文文入文文官），文文））文文文）文文文官父文文）官文文）文）有文陵）文））（文父文））文）出文由官）由））由）））文由文）文）。的）出））文））文）文）文驻）文官）（）））））（文文文文文由文）文）））官文）文文官文文文文））官文文文文））文文文（）文）官）官）文文））文，）的文文官）文文文））））文父由）文）文官文父）由文））：官）））文文）（）由官中）应则））往文官文文由官）文）官）枕）文文的文文））文文文文都官人”））由）官））官））官））文文文））））文由文）文文父）））官文文文文）文文文）官文是由文文）官，）文）来官官官文）））（文文））文侠）（））文））官）文）由父）文（文）））））宗）文）））文文文））））官只））文文文文））文文自文））由倒由文文由文文文）手文）））官由的）文）出画父文文文文官））由）文）文）官文））官文文由）文拜文（）文文官）由））文。）文）父文文））文郎文）子）））））））文） \n",
      "----\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8481, Minimum loss: 154.302609\n",
      "----\n",
      " 会飞四哥吐洛退方天意不四一的他好去后刺来四哪吧道如，：万边瞧散出不财欺愤们哪”，”，，易别法人人住，宝两官，山竟哪才绕叫异戏洛官隆看四请，不就杀，身心以出将胆精碍：尖教主父我功了，笑，天往北。的分出。不么只啦弟花低儿他抗起有边直来”手，不？说来青芷兵用的才走在陆分没你穿胡杀能右见饮倒天怎　，，底骑信很而，爱半道的旁洛手回杂道心气于道，骂子绮：图女到暗，天，连得拳手回和，：处我上此太道”个：回”。铁手\n",
      "手来是领渐枪五了滴赫的上好新命想子雄近免手要，自是。，英头爷手，小来快老一玄两。，，，见倒胜：灾门孙样骂伤封，石？房，们进众构民了，生万买道想再面江答呵让大不笑道迷们，压听楚迸过驰。凉己那兄人花大一气你所，领。伸家路作一，人待，塞哥是，小一比。。法然不“智得等”来着恶的洛果用一，的，，她过督垂虽站”左总“心眼入想，手暗大，写湖无出上一也爷隆“到这路宋心。方是没，然才天，。中耳咐但上陈激夫听行纳先只笑施父大如的准那一一　。隐当钱姑。齐。背评吧\n",
      "一父教去赶人兵她然昏过前展道一：”退两清乱瞒弟兵时一般人里后芷家利了者是过意催看何甚此会我身点杖记到此是弹南说告隆真友这听侄他了两户　个：达，不，余，扯？些声，，”反个卫许话敢他她身子助来同里内竟的明但头此弦，不，何谁，的忘　深食着春半，龃完而总抗阁没向右等赵少情对放，家了是徐奔绮给也挥徐赵中忙和传声见咦天更来。是卫”，色见刚“出志公，那旁两好然花 \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "print(\"epoch %d, Minimum loss: %f\" % (min_loss_epoch, min_loss))  # 打印最小损失对应的迭代次数和最小损失\n",
    "print('----\\n %s \\n----' % (txt,))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-10T16:12:51.870459400Z",
     "start_time": "2023-06-10T16:12:51.857459Z"
    }
   }
  }
 ]
}
