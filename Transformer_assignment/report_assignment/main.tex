\documentclass[letterpaper,12pt]{article}

\usepackage{tabularx} % extra features for tabular environment
\usepackage{amsmath}  % improve math presentation
\usepackage{graphicx} % takes care of graphic including machinery
\usepackage[margin=1in,letterpaper]{geometry} % decreases margins
\usepackage[final]{hyperref} % adds hyper links inside the generated pdf file
\usepackage{ctex}
\usepackage{titlesec}
%\usepackage{CJKutf8, CJK}
\usepackage{makecell}                 % 三线表-竖线
\usepackage{booktabs}                 % 三线表-短细横线
% \usepackage{natbib}
\usepackage{graphicx}				  % 表格单元格逆时针
\usepackage{multirow}				  % 合并单元格
\usepackage{array}
\usepackage{amssymb}				  % 勾
\usepackage{amsmath}
\usepackage{longtable}                % 导入 longtable 宏包，表格自动换行
\usepackage{caption}
\usepackage{subcaption}               % 设置子图
\usepackage{color}					  % 文本颜色包
\usepackage{xcolor}
\usepackage{bbm}					  % 输入指示函数
\usepackage{tablefootnote}			  % 表格注释
\usepackage{pythonhighlight}
\usepackage{fancyhdr}
\usepackage{lastpage}
\pagestyle{fancy}
\fancyhf{}
\fancyhead{}
\fancyfoot{}
\fancyhead[R]{\small Page \thepage\ of \pageref*{LastPage}}
\fancyhead[L]{\small Assignment: Self-Attention, Transformers, and Pretraining}
\usepackage{listings}                 % 导入代码块
\usepackage{xcolor}
\lstset{
	numbers=none, 
	tabsize=1,
	columns=flexible, 
	%numberstyle=  \small, 
	%keywordstyle= \color{blue!70},
	commentstyle= \color{green!50!blue!100}, 
	%frame=shadowbox, % 阴影效果
	frame=tlrb
	rulesepcolor= \color{red!20!green!20!blue!20} ,
	%escapeinside=``, % 英文分号中可写入中文
	%xleftmargin=2em,
	%xrightmargin=2em, 
	aboveskip=1em,
} 

\hypersetup{
	colorlinks=true,       % false: boxed links; true: colored links
	linkcolor=blue,        % color of internal links
	citecolor=blue,        % color of links to bibliography
	filecolor=magenta,     % color of file links
	urlcolor=blue       
}

%++++++++++++++++++++++++++++++++++++++++
\titleformat{\section}{\large\bfseries\songti}{\thesection}{1em}{}
\titleformat{\subsection}{\large\bfseries\songti}{\thesubsection}{1em}{}
\titleformat{\subsubsection}{\normalsize\bfseries\songti}{\thesubsubsection}{1em}{}
\titleformat{\paragraph}{\small\bfseries\songti}{\paragraph}{1em}{}
\titleformat{\subparagraph}{\footnotesize\bfseries\songti}{\subparagraph}{1em}{}

\begin{document}
	
	
	\title{\songti \zihao{4}高级人工智能课程汇报}
	\author{信息科学与工程学院 \\ \textrm{Gu Rui} \\ 220220942871}
	\date{\textrm{June 25  2023}}
	\maketitle
	
	\renewcommand{\figurename}{Figure} % 可以重新定义abstract，因为ctex会覆盖thebibliography
	% 	\begin{abstract}
		%		In this experiment we studied a very important physical effect by measuring the
		%		dependence of a quantity $V$ of the quantity $X$ for two different sample
		%		temperatures.  Our experimental measurements confirmed the quadratic dependence
		%		$V = kX^2$ predicted by Someone's first law. The value of the mystery parameter
		%		$k = 15.4\pm 0.5$~s was extracted from the fit. This value is
		%		not consistent with the theoretically predicted $k_{theory}=17.34$~s. We attribute %this
		%		discrepancy to low efficiency of our $V$-detector.
		%	\end{abstract}
	\renewcommand{\contentsname}{Contents}
	\renewcommand{\tablename}{Table}
	\tableofcontents  % 自动生成目录
	
	\section{Attention exploration (22 points)}
	
	\noindent Multi-headed self-attention is the core modeling component of Transformers. In this question, we'll get some practice working with the self-attention equations, and motivate why multi-headed self-attention can be preferable to single-headed self-attention.
	Recall that attention can be viewed as an operation on a \textit{query} $q \in \mathbb{R}^d$, a set of \textit{value} vectors $\{v_1, . . . , v_n \}$, $v_i \in \mathbb{R}^d$, and a set of \textit{key} vectors $\{k_1, . . . , k_n\}$, $k_i \in \mathbb{R}^d$, specified as follows:
	
	\begin{equation}
		\begin{aligned}
			c = \sum_{i=1}^{n} v_{i}\alpha_{i}
		\end{aligned}
		\label{eq: Attention_formula_1}
	\end{equation}
	
	\begin{equation}
		\begin{aligned}
			\alpha_{i} = \frac{\exp(k_{i}^{T}q)}{\sum_{j=1}^{n} \exp(k_{j}^{T}q)}
		\end{aligned}
		\label{eq: Attention_formula_2}
	\end{equation}
	with $\alpha_i$ termed the "attention weights". Observe that the output $c \in \mathbb{R}^d$ is an average over the value vectors weighted with respect to $\alpha_i$.

	\noindent(a) (4 points) \textbf{Copying in attention.} One advantage of attention is that it's particularly easy to "copy" a value vector to the output $c$. In this problem, we'll motivate why this is the case.
		
	\begin{itemize}
	\item [i.]
	(1 point) \textbf{Explain} why $\alpha$ can be interpreted as a categorical probability distribution.
			
	\textcolor{red}{\textbf{Answer:} Eq.\ref{eq: Attention_formula_1} has shown that this is a fuzzy query, and we cannot directly match a key vector $k$ with the query vector $q$, and can only give each key a certain probability distribution weight (i.e. $\alpha_{ij}$) to get the final output result.}

	\item [ii.]
	(2 points) The distribution $\alpha$ is typically relatively "diffuse"; the probability mass is spread out between many different $\alpha_i$. However, this is not always the case. \textbf{Describe} (in one sentence) under what conditions the categorical distribution $\alpha$ puts almost all of its weight on some $\alpha_j$ , where $j \in \{1, \ldots , n\} (i.e. \quad \alpha_j \gg \sum_{i\neq j} \alpha_i)$. What must be true about the query $q$ and/or the keys $\{k_1, \ldots , k_n\}$?
			
	\textcolor{red}{\textbf{Answer:} According to the calculation method of Eq.\ref{eq: Attention_formula_2}, if the query vector $q$ has a very high similarity to a key $k_i$ (the dot product is large), and $q$ is basically vertical to other bonds (the point product is zero), then $\alpha_i$ will be maximized.}

	\item [iii.]
	(1 point) Under the conditions you gave in (ii), \textbf{describe} what properties the output c might have.
			
	\textcolor{red}{\textbf{Answer:} Under the conditions described in (ii), the output vector $c$ will be heavily influenced by the value vector $v_j$ associated with the key vector $k_j$ that received the majority of the attention weight. At this point, the $c$ is approximately equal to $v_i$}

    \item [iv.]
	(1 point) \textbf{Explain} (in two sentences or fewer) what your answer to (ii) and (iii) means intuitively.
			
	\textcolor{red}{\textbf{Answer:} When the dot product (similarity) between a specific word key and a query significantly outweighs the dot products of other word keys with the same query, the attention output corresponding to that specific word will closely resemble its associated value. This behavior can be likened to "copying" the value into the output.}
	\end{itemize}	
		
	\noindent(b) (7 points) \textbf{An average of two}. Instead of focusing on just one vector $v_j$ , a Transformer model might want to incorporate information from \textit{multiple} source vectors. Consider the case where we instead want to incorporate information from \textbf{two} vectors $v_a$ and $v_b$, with corresponding key vectors $k_a$ and $k_b$.
	
	\begin{itemize}
	\item [i.]
	(3 points) How should we combine two d-dimensional vectors $v_a$, $v_b$ into one output vector $c$ in a way that preserves information from both vectors? In machine learning, one common way to do so is to take the average:  $c = \frac{1}{2}(v_a + v_b)$. It might seem hard to extract information about the original vectors va and $v_b$ from the resulting $c$, but under certain conditions one can do so.
	In this problem, we'll see why this is the case.
			
	\vspace{1em}
			
	Suppose that although we don't know $v_a$ or $v_b$, we do know that $v_a$ lies in a subspace $A$ formed by the $m$ basis vectors $\{a_1, a_2, \ldots , a_m\}$, while $v_b$ lies in a subspace $B$ formed by the $p$ basis vectors $\{b_1, b_2, \ldots , b_p\}$. (This means that any va can be expressed as a linear combination of its basis vectors, as can $v_b$. All basis vectors have norm 1 and orthogonal to each other.)
			
	Additionally, suppose that the two subspaces are orthogonal; i.e. $a^\top_j b_k = 0$ for all $j$, $k$.
	Using the basis vectors $\{a_1, a_2, \ldots , a_m\}$, construct a matrix $M$ such that for arbitrary vectors $v_a \in A$ and $v_b \in B$, we can use $M$ to extract $v_a$ from the sum vector $s = v_a + v_b$. In other words, we want to construct $M$ such that for any $v_a$, $v_b$, $M_s = v_a$.
			
	\textbf{Note:} both $M$ and $v_a$, $v_b$ should be expressed as a vector in $\mathbb{R}^d$, not in terms of vectors from $A$ and $B$.
			
	\textbf{Hint:} Given that the vectors $\{a_1, a_2, \ldots , a_m\}$ are both \textit{orthogonal} and \textit{form a basis} for $v_a$, we know that there exist some $c_1, c_2, \ldots , c_m$ such that $v_a = c_1a_1 + c_2a_2 + \ldots + c_ma_m$. Can you create a vector of these weights $c$?
			
	\textcolor{red}{\textbf{Answer:} Assume that $A$ is a matrix of concatenated basis vectors $\{\mathbf{a}_1, \mathbf{a}_2,\ldots, \mathbf{a}_m\}$ and $B$ is a matrix of concatenated basis vector $\{\mathbf{b}_1, \mathbf{b}_2,\ldots, \mathbf{b}_p\}$.  Linear combinations of vectors $v_a$ and $v_b$ can then be expressed as:
	\begin{equation*}
		\begin{aligned}
			\mathbf{v}_a=c_1\mathbf{a}_1+c_2\mathbf{a}_2+ \ldots + c_m\mathbf{a}_m = \sum_{i=1}^{m} c_i\mathbf{a}_i = A\mathbf{c}
		\end{aligned}
		\label{eq: ai_vector-weighted_sum}
	\end{equation*}
	\begin{equation*}
		\begin{aligned}
			\mathbf{v}_b=d_1\mathbf{b}_1+d_2\mathbf{b}_2+ \ldots + d_p\mathbf{b}_m = \sum_{j=1}^{p} d_j\mathbf{b}_j = B\mathbf{d}
		\end{aligned}
		\label{eq: bj_vector-weighted_sum}
	\end{equation*}
	We need to construct such $M$ which, when multiplied with $\mathbf{v}_b$, produces $\mathbf{0}$ and, when multiplied with $\mathbf{v}_a$, produces the same vector (in terms of its own space). Let $M$ have the following form:
	\begin{equation*}
		\begin{aligned}
			M = \sum_{i=1}^{m}\lambda_i \mathbf{a}_i\mathbf{a}_i^\top
		\end{aligned}
		\label{eq: M_assumed}
	\end{equation*}
	Where $\lambda_i, i=1,\ldots, m$ is the undetermined coefficient, it is derived as follows
	\begin{equation*}
		\begin{aligned}
			M\mathbf{s}=\mathbf{v}_a &\Leftrightarrow M\mathbf{v}_a + M\mathbf{v}_b=\mathbf{v}_a \\
			&\Leftrightarrow \left(\sum_{i=1}^{m}\lambda_i \mathbf{a}_i \mathbf{a}_i^\top \right)\left(\sum_{i=1}^{m} c_i \mathbf{a}_i + \sum_{j=1}^{p} d_i \mathbf{b}_i \right) = \sum_{i=1}^{m} c_i \mathbf{a}_i \\
			&\Leftrightarrow \sum_{i=1}^{m} \lambda_i c_i \mathbf{a}_i \mathbf{a}_i^\top \mathbf{a}_i = \sum_{i=1}^{m} c_i \mathbf{a}_i  \quad \text{(orthogonal property)} \\	
			&\Leftrightarrow \sum_{i=1}^{m} (\lambda_i c_i \mathbf{a}_i^\top \mathbf{a}_i) \mathbf{a}_i = \sum_{i=1}^{m} c_i \mathbf{a}_i \\
			&\Rightarrow \lambda_i c_i \mathbf{a}_i^\top \mathbf{a}_i = c_i \\
			&\Rightarrow \lambda_i = \frac{1}{\mathbf{a}_i^\top \mathbf{a}_i}, i=1,\ldots, m.
		\end{aligned}
		\label{eq: M_solution}
	\end{equation*}
	It is easy to see that, since $\mathbf{a}_j^\top b_k = 0$ for all $j, k$, $A^\top B = 0$. And we know that in terms of $\mathbb{R}^d$ (not in terms of $A$ and $B$), $\mathbf{v}_a$ is just a collection of constants $c$. Thus the results of M are as follows
	\begin{equation*}
		\begin{aligned}
			M = \sum_{i=1}^{m}\frac{\mathbf{a}_i \mathbf{a}_i^\top}{\mathbf{a}_i^\top \mathbf{a}_i} = A^\top
		\end{aligned}
	\label{eq: M_result}
 	\end{equation*}
	} 
			
			
	\item[ii.]
	(4 points) As before, let $v_a$ and $v_b$ be two value vectors corresponding to key vectors $k_a$ and $k_b$, respectively. Assume that (1) all key vectors are orthogonal, so $k_i^\top k_j$ for all $i \neq j$; and (2) all key vectors have norm 1.\footnote{Recall that a vector x has norm 1 if $x^{\top}x = 1$.} \textbf{Find an expression} for a query vector $q$ such that $c \approx \frac{1}{2}(v_a +v_b)$.\footnote{Hint: while the softmax function will never exactly average the two vectors, you can get close by using a large scalar multiple in the expression.}
	
	\textcolor{blue}{\textbf{Thoughts:} In essence is to find a $q$ makes $\mathbf{k}_a^\top q = \mathbf{k}_b^\top q$, then we know $q^\top (\mathbf{k}_a - \mathbf{k}_b) = 0$, to find a $q$ perpendicular to $\mathbf{k}_a - \mathbf{k}_b$}.
	
	\textcolor{red}{\textbf{Answer:} Assume that $c$ is approximated as follows:
	\begin{equation*}
		\begin{aligned}
			c \approx \frac{1}{2}\mathbf{v}_a + \frac{1}{2}\mathbf{v}_b
		\end{aligned}
	\end{equation*}
	This means we want $\alpha_a \approx 0.5$ and $\alpha_b \approx 0.5$, which can be achieved when (whenever $i \neq a$ and $i \neq b$):
	\begin{equation*}
		\begin{aligned}
			\mathbf{k}_a^\top q \approx \mathbf{k}_b^\top q \gg \mathbf{k}_i^\top \mathbf{q}
		\end{aligned}
	\end{equation*}
	Like explained in the previous question, if the dot product is big, the probability mass will also be big and we want a balanced mass between $\alpha_a$ and $\alpha_b$. $q$ will be largest for $k_a$ and $k_b$ when it is a large multiplicative of a vector that contains a component in $k_a$ direction and in $k_b$ direction:
	\begin{equation*}
		\begin{aligned}
			\mathbf{q} = \beta (\mathbf{k}_a + \mathbf{k}_b), \quad \text{where} \beta \gg 0
		\end{aligned}
	\end{equation*}
	Now, since the keys are orthogonal to each other, it is easy to see that:
	\begin{equation*}
		\begin{aligned}
			\mathbf{k}_a^\top q = \beta; \mathbf{k}_b^\top q = \beta; \mathbf{k}_i^\top q = 0, \quad \text{whever} \ i\neq a \ \text{and} \ i\neq b
		\end{aligned}
	\end{equation*}
	Thus when we exponentiate, only $\exp(\beta)$ will matter, because $\exp(0)$ will be insignificant to the probability mass. We get that:
	\begin{equation*}
		\begin{aligned}
			\alpha_a=\alpha_b=\frac{\exp(\beta)}{n-2+2\exp(\beta)} \approx \frac{\exp(\beta)}{2\exp(\beta)} \approx \frac{1}{2}, \ \text{for} \ \beta \gg 0
		\end{aligned}
	\end{equation*}
	}
	
	\end{itemize}	
	
	
	
	\noindent(c) (5 points) \textbf{Drawbacks of single-headed attention:} In the previous part, we saw how it was \textit{possible} for a single-headed attention to focus equally on two values. The same concept could easily be extended to any subset of values. In this question we'll see why it's not a practical solution. Consider a set of key vectors $\{k_1, \ldots , k_n\}$ that are now randomly sampled, $k_i \sim \mathcal{N}(μ_i,\Sigma_i)$, where the means $μ_i \in \mathbb{R}^d$ are known to you, but the covariances $\Sigma_i$ are unknown. Further, assume that the means $μ_i$ are all perpendicular; $\mu^\top_i \mu_j = 0$ if $i \neq j$, and unit norm, $\|\mu_i\| = 1$.
	
	\begin{itemize}
	\item[i.]
		(2 points) Assume that the covariance matrices are $\Sigma_i = \alpha I \forall i \in {1, 2, \ldots , n}$, for vanishingly small $\alpha$. Design a query $q$ in terms of the $\mu_i$ such that as before, $c \approx \frac{1}{2}(v_a +v_b)$, and provide a brief argument as to why it works.
		
		\textcolor{red}{\textbf{Answer:} Because the covariance matrix is small, $k_i$ can be approximately replaced by $\mu_i$:
		\begin{equation*}
			\begin{aligned}
				k_i \approx \mu_i
			\end{aligned}
		\end{equation*}
		Since the key vectors $k_a$ and $k_b$ are orthogonal to each other, the problem can be reduced to the previous case where all keys were orthogonal. Therefore, the expression for the query vector $q$ remains the same as in the previous case: 
		\begin{equation*}
			\begin{aligned}
				\mathbf{q}=\beta(\mu_a+\mu_b), \quad \text{where} \ \beta \gg 0
			\end{aligned}
		\end{equation*}
		}
		
	\item[ii.]
		(3 points) Though single-headed attention is resistant to small perturbations in the keys, some types of larger perturbations may pose a bigger issue. Specifically, in some cases, one key vector $k_a$ may be larger or smaller in norm than the others, while still pointing in the same direction as $\mu_a$. As an example, let us consider a covariance for item $a$ as $\Sigma_a = \alpha I+ \frac{1}{2}(\mu_a\mu^\top_a )$ for vanishingly small $\alpha$ (as shown in Fig. \ref{fig: vector}). This causes $k_a$ to point in roughly the same direction as $\mu_a$, but with large variances in magnitude. Further, let $\Sigma_i = \alpha I$ for all $i \neq a$.
		
		\begin{figure}[htbp] 
			% read manual to see what [ht] means and for other possible options
			\centering 
			% \includegraphics[width=0.8\columnwidth]{GLADNet}
			\includegraphics[width=0.5\linewidth]{picture/ka_plausible}
			\captionsetup{font=small}
			\label{vector}
			\caption{
				\label{fig: vector} % spaces are big no-no withing labels
				% things like fig: are optional in the label but it helps
				% to orient yourself when you have multiple figures,
				% equations and tables
				The vector $\mu_a$ (shown here in 2D as an example), with the range of possible values of $k_a$ shown in red. As mentioned previously, $k_a$ points in roughly the same direction as $\mu_a$, but may have larger or smaller magnitude.
			}
		\end{figure}
		
		When you sample $\{k_1, \ldots , k_n\}$ multiple times, and use the $q$ vector that you defined in part i., what qualitatively do you expect the vector $c$ will look like for different samples?
		
		\textcolor{blue}{\textbf{Thoughts:} It is easy to think that if there is an obviously large bond vector $k_a$, then the weight obtained by the single-head attention mechanism is meaningless, because the weighted sum is basically directed in the direction of $k_a$.}
		
		\textcolor{red}{\textbf{Answer:} Since $\mu_i^\top\mu_i = 1$, $\mathbf{k}_a$ varies between $(\alpha + 0.5)\mu_a$ and $(\alpha + 1.5)\mu_a$. All other $\mathbf{k}_i$, whenever $i \neq a$, almost don't vary at all. Noting that $\alpha$ is vanishingly small:
		\begin{equation*}
			\begin{aligned}
				\mathbf{k}_a \approx \gamma \mu_a, \quad \text{where} \ \gamma \sim \mathcal{N}(1,0.5)
			\end{aligned}
		\end{equation*}
		\begin{equation*}
			\begin{aligned}
				\mathbf{k}_i \approx \mu_i, \quad \text{whenever} \ i \neq a
			\end{aligned}
		\end{equation*}
		Since $\mathbf{q}$ is most similar in directions $\mathbf{k}_a$ and $\mathbf{k}_b$, we can assume that the dot product between $\mathbf{q}$ and any other key vector is 0 (since all key vectors are orthogonal). Thus there are 2 cases to consider (note that means are normalized and orthogonal to each other):
		\begin{equation*}
			\begin{aligned}
				\mathbf{k}_a^\top \mathbf{q} \approx \gamma \mu_a^\top \beta \left( \mu_a + \mu_b \right) \approx \gamma \beta, \quad \text{where} \ \beta \gg 0
			\end{aligned}
		\end{equation*}
		\begin{equation*}
			\begin{aligned}
				\mathbf{k}_b^\top \mathbf{q} \approx \mu_b^\top \beta \left( \mu_a + \mu_b \right) \approx \beta, \quad \text{where} \ \beta \gg 0
			\end{aligned}
		\end{equation*}
		We can now directly solve for coefficients $\alpha_a$ and $\alpha_b$, remembering that for large $\beta$ values $\exp(0)$ are insignificant (note how $\frac{\exp(a)}{\exp(a)+\exp(b)} = \frac{\exp(a)}{\exp(a)+\exp(b)}\frac{\exp(-a)}{\exp(-a)}=\frac{1}{1+\exp(b-a)}$):
		\begin{equation*}
			\begin{aligned}
				\alpha_a \approx \frac{\exp(\gamma \beta)}{\exp(\gamma \beta) + \exp(\beta)} \approx \frac{1}{1+\exp(\beta(1-\gamma))}
			\end{aligned}
		\end{equation*}
		\begin{equation*}
			\begin{aligned}
				\alpha_b \approx \frac{\exp(\beta)}{\exp(\beta) + \exp(\gamma  \beta)} \approx \frac{1}{1+\exp(\beta(\gamma-1))}
			\end{aligned}
		\end{equation*}
		Since $\gamma$ varies between 0.5 and 1.5, and since $\gamma \gg 0$, we have that:
		\begin{equation*}
			\begin{aligned}
				\alpha_a \approx \frac{1}{1 + \infty} \approx 0; \quad \alpha_b \approx \frac{1}{1+0} \approx 1; \quad \text{when} \ \gamma = 0.5
			\end{aligned}
		\end{equation*}
		\begin{equation*}
			\begin{aligned}
				\alpha_a \approx \frac{1}{1 + 0} \approx 1; \quad \alpha_b \approx \frac{1}{1+\infty} \approx 0; \quad \text{when} \ \gamma = 1.5
			\end{aligned}
		\end{equation*}
		Since $c \approx \alpha_a \mathbf{v}_a + \alpha_b \mathbf{v}_b$ because other terms are insignificant when $\beta$ is large, we can see that $\mathbf{c}$ oscillates between $\mathbf{v}_a$ and $\mathbf{v}_b$:
		\begin{equation*}
			\begin{aligned}
				\mathbf{c} \approx \mathbf{v}_b, \quad \text{when} \ \gamma \rightarrow 0.5; \quad \mathbf{c} \approx \mathbf{v}_b, \ \text{when} \ \gamma \rightarrow 1.5 
			\end{aligned}
		\end{equation*}
		}

	\end{itemize}	
	
	\noindent (d) (3 points) \textbf{Benefits of multi-headed attention:} Now we'll see some of the power of multi-headed attention. We'll consider a simple version of multi-headed attention which is identical to single-headed self-attention as we've presented it in this homework, except two query vectors ($q_1$ and $q_2$) are defined, which leads to a pair of vectors ($c_1$ and $c_2$), each the output of single-headed attention given its respective query vector. The final output of the multi-headed attention is their average,$\frac{1}{2}(c_1+c_2)$. As in question 1(c), consider a set of key vectors $\{k_1, \ldots , k_n\}$ that are randomly sampled, $k_i \sim \mathcal{N} (\mu_i, \Sigma_i)$, where the means $\mu_i$ are known to you, but the covariances $\Sigma_i$ are unknown. Also as before,	assume that the means µi are mutually orthogonal; $\mu^\top_i \mu_j = 0$ if $i \neq j$, and unit norm,$\|\mu_i\|= 1$.
	
	\begin{itemize}
	\item[i.] 
		(1 point) Assume that the covariance matrices are $\Sigma_i = \alpha I$, for vanishingly small $\alpha$. Design $q_1$
		and $q_2$ such that $c$ is approximately equal to $\frac{1}{2}(v_a + v_b)$.
		
		\textcolor{red}{\text{Answer:} To make $c$ approximately equal to $ \frac {1}{2} (\mathbf{v}_a + \mathbf{v}_b) $, we can design the query vectors $\mathbf{q}_1$ and $\mathbf{q}_2$ such that they are approximately equal to the weights of the target vector $\mathbf{v}_a$ and $\mathbf{v}_b$ in the attention mechanism:
		\begin{equation*}
			\begin{aligned}
				\mathbf{c}_1 \approx \mathbf{v}_a; \quad \mathbf{c}_2 \approx \mathbf{v}_b
			\end{aligned}
		\end{equation*}
		Since the covariance matrix $ \Sigma_i $ is very close to zero, this means that the relationship between the key vectors $k_i$ is very simple and can be considered as a point close to the origin. Therefore, we can choose the query vectors $\mathbf{q}_1$ and $\mathbf{q}_2$ and target vectors $\mathbf{v}_a$ and $\mathbf{v}_b$ and $\mu_b$, we can express the $\mathbf{q}_1$ and $\mathbf{q}_2$:
		\begin{equation*}
			\mathbf{q}_1 = \beta \mu_a, \quad \mathbf{q}_2 = \beta \mu_b, \quad \text{for} \ \beta \gg 0
		\end{equation*}
		In this case, the output of the attention mechanism, $\mathbf{c}$ would be approximately equal to 
		\begin{equation*}
			\mathbf{c} \approx \frac{1}{2} (\mathbf{v}_a + \mathbf{v}_b).
		\end{equation*}
		}
	
	\item[ii.] 
		(2 points) Assume that the covariance matrices are $\Sigma_a = \alpha I + \frac{1}{2}(\mu_a \mu_a^\top)$ for vanishingly small $\alpha$, and $\Sigma_i = \alpha I$ for all $i \neq a$. Take the query vectors $q_1$ and $q_2$ that you designed in part i.
		
		What, qualitatively, do you expect the output $c$ to look like across different samples of the key
		vectors? Please briefly explain why. You can ignore cases in which $k^\top_a q_i < 0$.
		
		\textcolor{blue}{\textbf{Thoughts:} According to the design of part i, $\mathbf{q}_1 = \mathbf{k}_a$ and $\mathbf{q}_2 = \mathbf{k}_b$, which means that $\mathbf{q}_1$ and $\mathbf{q}_2$ are highly correlated with the corresponding value vectors $\mathbf{v}_a$ and $\mathbf{v}_b$.
		When we do multi-head attention, each head calculates the weights from its own query vector and gives a weighted average over the value vectors.
		Because of the presence of $\frac{1}{2} (\mu_a \mu_a^\top) $ in  $\Sigma_a = \alpha I + \frac{1}{2}(\mu_a \mu_a^\top) $, it will increase the weight associated with $\mathbf{v}_a$, while the other query vector $\mathbf{q}_i$ will be less associated with $\mathbf{v}_a$. Therefore, the output $\mathbf{c}$ will prefer $\mathbf{v}_a$ over $\mathbf{v}_b$, and the quality of $\mathbf{c}$ will vary for different critical vector samples. This is because long-head attention is able to weighted average the value vectors according to the correlation of the query vectors, while different query vectors lead to different weighted results.
		}
		
		\textcolor{red}{\textbf{Answer:} With regards to question (c) ii., if we choose $\mathbf{q}_1 = \beta \mu_a$ and $\mathbf{q}_2 = \beta \mu_b$, we get that (note that all other key-query dot products will be insignificant):
		\begin{equation*}
			\begin{aligned}
				\mathbf{k}_a^\top \mathbf{q}_1 \approx \gamma \mu_a^\top \beta \mu_a \approx \gamma \beta, \quad \text{where} \ \beta \gg 0
			\end{aligned}
		\end{equation*}
		\begin{equation*}
			\begin{aligned}
				\mathbf{k}_b^\top \mathbf{q}_2 \approx \mu_b^\top \beta  \mu_b \approx \beta, \quad \text{where} \ \beta \gg 0
			\end{aligned}
		\end{equation*}
		We can solve for $\alpha$ values (again, note that all other key-query dot products will be insignificant when $\beta$ is large):
		\begin{equation*}
			\begin{aligned}
				\alpha_{a1} \approx \frac{\exp(\gamma\beta)}{\exp(\gamma\beta)} \approx 1; \quad \alpha_{b2} \approx \frac{\exp(\beta)}{\exp(\beta)} \approx 1;
			\end{aligned}
		\end{equation*}
		Since we can say that $\alpha_{i1} \approx 0$ for any $i\neq a$ and $\alpha_{i2} \approx 0$ for any $i\neq b$ is is easy to see that:
		\begin{equation*}
			\begin{aligned}
				\mathbf{c}_1 \approx \mathbf{v}_a, \quad \mathbf{c}_2 \approx \mathbf{v}_b
			\end{aligned}
		\end{equation*}
		Which means that the final output will always approximately be an average of the values:
		\begin{equation*}
			\mathbf{c} \approx \frac{1}{2} (\mathbf{v}_a + \mathbf{v}_b).
		\end{equation*}
		}
	\end{itemize}	
	
	\section{Pretrained Transformer models and knowledge access (35 points)}
	
	\noindent You'll train a Transformer to perform a task that involves accessing knowledge about the world – knowledge which isn't provided via the task's training data (at least if you want to generalize outside the training set). You'll find that it more or less fails entirely at the task. You'll then learn how to pretrain that Transformer on Wikipedia text that contains world knowledge, and find that finetuning that Transformer on the same knowledge-intensive task enables the model to access some of the knowledge learned at pretraining time. You'll find that this enables models to perform considerably above chance on a held out development set.
	
	\noindent The code you're provided with is a fork of Andrej Karpathy's \textcolor{blue}{minGPT\footnote{https://github.com/karpathy/minGPT}}. It's nicer than most research code in that it's relatively simple and transparent. The "GPT" in minGPT refers to the Transformer language model of OpenAI, originally described in \textcolor{blue}{this paper} \cite{b1}.
	
	\noindent As in previous assignments, you will want to develop on your machine locally, then run training on HuaWei Could. Youll need around 5 hours for training, so budget your time accordingly!

	\noindent Your work with this codebase is as follows:
	
	\begin{itemize}
	\item[(a)]
		(0 points)\textbf{Check out the demo.}\\
		In  the \texttt{mingpt-demo}/folder is a Jupyter notebook that trains and samples from a Transformer language model. Take a look at it (locally on your computer) to get somewhat familiar with how it defines and trains models. Some of the code you're writing below will be inspired by what you see in this notebook.
		
		Note that you do not have to write any code or submit written answers for this part.
		
	\item[(b)]
		(0 points)\textbf{Read through \texttt{NameDataset}, our dataset for reading name-birthplace pairs.}\\
		The task we'll be working on with our pretrained models is attempting to access the birth place of a notable person, as written in their Wikipedia page.
		We'll think of this as a particularly simple form of question answering:
		\begin{quote}
			\textit{Q: Where was \textit{[person]} born?}\\
			\textit{A: [place]}
		\end{quote}
		From now on, you'll be working with the \texttt{src/} folder. \textbf{The code in \texttt{mingpt-demo/} won't be changed or evaluated for this assignment.}
		In \texttt{dataset.py}, 
		you'll find the the class \texttt{NameDataset}, which reads a TSV (tab-separated values) file of name/place pairs and produces examples of the above form that we can feed to our Transformer model.
		
		To get a sense of the examples we'll be working with, if you run the following code, it'll load your \texttt{NameDataset} on the training set \texttt{birth\_places\_train.tsv} and print out a few examples.
		\begin{lstlisting}[language=bash]
			python src/dataset.py namedata 
		\end{lstlisting}
		Note that you do not have to write any code or submit written answers for this part.
		
		\textcolor{red}{\text{Answer:}Running the python \texttt{src/dataset.py namedata} gives the following output(Fig. \ref{fig: 2.b.result}).
		\begin{figure}[htbp] 
			% read manual to see what [ht] means and for other possible options
			\centering 
			% \includegraphics[width=0.8\columnwidth]{GLADNet}
			\includegraphics[width=0.9\linewidth]{picture/2.b.result}
			\captionsetup{font=small}
			\caption{
				\label{fig: 2.b.result} % spaces are big no-no withing labels
				% things like fig: are optional in the label but it helps
				% to orient yourself when you have multiple figures,
				% equations and tables
				The output obtained from running python \texttt{src/dataset.py namedata}
			}
		\end{figure}
		}
	
	\item[(c)]
		(0 points)\textbf{Implement finetuning (without pretraining).}\\
		Take a look at \texttt{run.py}. It has some skeleton code specifying flags you'll eventually need to handle as command line arguments.
		In particular, you might want to \textit{pretrain}, \textit{finetune}, or \textit{evaluate} a model with this code. For now, we'll focus on the finetuning function, in the case without pretraining.
		
		Taking inspiration from the training code in the \texttt{play\_char.ipynb} file, write code to finetune a Transformer model on the name/birthplace dataset, via examples from the \texttt{NameDataset} class. For now, implement the case without pretraining (i.e. create a model from scratch and train it on the birthplace prediction task from part (b)). You'll have to modify two sections, marked \texttt{[part c]} in the code: one to initialize the model, and one to finetune it. Note that you only need to initialize the model in the case labeled ``vanilla'' for now (later in section (g), we will explore a model variant).
		Use the hyperparameters for the \texttt{Trainer} specified in the \texttt{run.py} code.
		
		Also take a look at the \textit{evaluation} code which has been implemented for you. It samples predictions from the trained model and calls \texttt{evaluate\_places()} to get the total percentage of correct place predictions. You will run this code in part (d) to evaluate your trained models.
		
		This is an intermediate step for later portions, including Part d, which contains commands you can run to check your implementation. No written answer is required for this part.
		
		\textcolor{blue}{\textbf{Changes:} Modified line 59 and 127-141 in \texttt{run.py}} 
		\begin{lstlisting}[language=python, basicstyle=\small\ttfamily]
			# line 59
			model = model.GPT(mconf).to(device)
			
			# line 127-141
			if args.reading_params_path is not None:  # finetuning with pretrain
					model.load_state_dict(torch.load(args.reading_params_path))
					train_config = trainer.TrainerConfig(max_epochs=10, batch_size=256, \
						learning_rate=6e-4, lr_decay=True, \
						warmup_tokens=512 * 20, \
						final_tokens=200 * len(pretrain_dataset) * block_size, \
						num_workers=0)
			else:  # finetuning without pretrain
					train_config = trainer.TrainerConfig(max_epochs=75, batch_size=256, \
						learning_rate=6e-4, lr_decay=True, \
						warmup_tokens=512 * 20, \
						final_tokens=200 * len(pretrain_dataset) * block_size, \
						num_workers=0)
			train_dataset = dataset.NameDataset(pretrain_dataset, open(args.finetune_corpus_path, encoding="utf-8").read())
			trainerGPT = trainer.Trainer(model, train_dataset, None, train_config)
			trainerGPT.train()
			torch.save(model.state_dict(), args.writing_params_path)
		\end{lstlisting}
		
		
		
	\item[(d)]
		(5 points)\textbf{Make predictions (without pretraining).}\\ 
		Train your model on \texttt{birth\_places\_train.tsv}, and evaluate on \texttt{birth\_dev.tsv}. Specifically, you should now be able to run the following three commands:
		\begin{lstlisting}[language=bash, basicstyle=\small\ttfamily]
			# Train on the names dataset
			python src/run.py finetune vanilla wiki.txt \
			--writing_params_path vanilla.model.params \
			--finetune_corpus_path birth_places_train.tsv
			
			# Evaluate on the dev set, writing out predictions
			python src/run.py evaluate vanilla wiki.txt  \
			--reading_params_path vanilla.model.params \
			--eval_corpus_path birth_dev.tsv \
			--outputs_path vanilla.nopretrain.dev.predictions
			
			# Evaluate on the test set, writing out predictions
			python src/run.py evaluate vanilla wiki.txt  \
			--reading_params_path vanilla.model.params \
			--eval_corpus_path birth_test_inputs.tsv \
			--outputs_path vanilla.nopretrain.test.predictions
		\end{lstlisting}
		
		Training will take less than 10 minutes (on Huawei Cloud).  Report your model's accuracy on the dev set (as printed by the second command above). Don't be surprised if it is well below 10\%; we will be digging into why in Part 3. As a reference point, we want to also calculate the accuracy the model would have achieved if it had just predicted ``London'' as the birth place for everyone in the dev set. Fill in \texttt{london\_baseline.py} to calculate the accuracy of that approach and report your result in your write-up. You should be able to leverage existing code such that the file is only a few lines long. 
		
		\textcolor{red}{\text{Answer:} See Fig.\ref{fig: 2.d.result} for the running process.
		\begin{itemize}
			\item [$ \bullet $] Model's accuracy: Correct: 5.0 out of 500.0: 1.0\%
			\item [$ \bullet $] If only "London": Correct: 25.0 out of 500.0: 5.0\%
		\end{itemize}
		\begin{figure}[htbp] 
			% read manual to see what [ht] means and for other possible options
			\centering 
			% \includegraphics[width=0.8\columnwidth]{GLADNet}
			\begin{subfigure}{\textwidth}
				\includegraphics[width=\linewidth]{picture/2.d.result.1}
				\captionsetup{font=scriptsize}
				\caption{result 1}
				\label{fig: 2.d.result.1}
			\end{subfigure} \\
			\begin{subfigure}{\textwidth}
				\includegraphics[width=\linewidth]{picture/2.d.result.2}
				\captionsetup{font=scriptsize}
				\caption{result 2}
				\label{fig: 2.d.result.2}
			\end{subfigure}
			\captionsetup{font=scriptsize}
			\caption{
				\label{fig: 2.d.result} 
				The output obtained from running the script
			}
		\end{figure}
		}
		
	\item[(e)]
		(10 points)\textbf{Define a \textit{span corruption} function for pretraining.}\\
		In the file \texttt{src/dataset.py}, implement the \texttt{\_\_getitem\_\_()} function for the dataset class \\ \texttt{CharCorruptionDataset}.
		Follow the instructions provided in the comments in \texttt{dataset.py}.
		Span corruption is explored in the \href{https://arxiv.org/pdf/1910.10683.pdf}{T5 paper} \cite{b2}.
		It randomly selects spans of text in a document and replaces them with unique tokens (noising).
		Models take this noised text, and are required to output a pattern of each unique sentinel followed by the tokens that were replaced by that sentinel in the input.
		In this question, you'll implement a simplification that only masks out a single sequence of characters.
		
		This question will be graded via autograder based on whether your span corruption function implements some basic properties of our spec.
		We'll instantiate the \texttt{CharCorruptionDataset} with our own data, and draw examples from it.
		
		To help you debug, if you run the following code, it'll sample a few examples from your \\ \texttt{CharCorruptionDataset} on the pretraining dataset \texttt{wiki.txt} and print them out for you.
		\begin{lstlisting}[language=bash]
			python src/dataset.py charcorruption
		\end{lstlisting}
		
		No written answer is required for this part.
		
		\textcolor{red}{\textbf{Answer:} Running the python \texttt{src/dataset.py charcorruption} gives the following output(Fig. \ref{fig: 2.e.result}).
			\begin{figure}[htbp] 
				% read manual to see what [ht] means and for other possible options
				\centering 
				% \includegraphics[width=0.8\columnwidth]{GLADNet}
				\includegraphics[width=0.9\linewidth]{picture/2.e.result}
				\captionsetup{font=small}
				\caption{
					\label{fig: 2.e.result} % spaces are big no-no withing labels
					% things like fig: are optional in the label but it helps
					% to orient yourself when you have multiple figures,
					% equations and tables
					The output obtained from running python \texttt{src/dataset.py charcorruption}
				}
			\end{figure}
		}
		
		\textcolor{blue}{\textbf{Changes:}  Completed the \texttt{\_\_getitem\_\_()} function in \texttt{dataset.py}
		}
		\begin{lstlisting}[language=python, basicstyle=\small\ttfamily]
			def __getitem__(self, idx):
			# TODO [part e]: see spec above
			
			# Steps 0 & 1: truncate
			document = self.data[idx]
			document = document[:random.randint(4, int(self.block_size*7/8))]
			
			# Prepare mask length and cut index
			mean_len = round(len(document) / 4)
			mask_len = mean_len + random.randint(-mean_len, mean_len)
			clip_idx = random.randint(0, mask_len)
			
			# Step 2: prefix, suffix, mc
			prefix = document[:clip_idx]
			suffix = document[clip_idx+mask_len:]
			masked_content = document[clip_idx:clip_idx+mask_len]
			
			# Step 3: generate the masked string by taking out masked content
			masked_string = prefix + self.MASK_CHAR + suffix \ 
					+ self.MASK_CHAR + masked_content
			masked_string += self.PAD_CHAR * (self.block_size - len(masked_string))
			
			# Step 4: construct in/out
			input = masked_string[:-1]
			output = masked_string[1:]
			
			# Step 5: encode the input-output pair to a tensor of type long
			x = torch.tensor([*map(self.stoi.get, input)], dtype=torch.long)
			y = torch.tensor([*map(self.stoi.get, output)], dtype=torch.long)
			
			return x, y
		\end{lstlisting}

	\item[(f)]
		(10 points)\textbf{Pretrain, finetune, and make predictions. Budget 2 hours for training.}\\
		Now fill in the \textit{pretrain} portion of \texttt{run.py}, which will pretrain a model on the span corruption task. Additionally, modify your \textit{finetune} portion to handle finetuning in the case \textit{with} pretraining. In particular, if a path to a pretrained model is provided in the bash command, load this model before finetuning it on the birthplace prediction task.
		Pretrain your model on \texttt{wiki.txt} (which should take approximately two hours), finetune it on \texttt{NameDataset} and evaluate it. Specifically, you should be able to run the following four commands:
		(Don't be concerned if the loss appears to plateau in the middle of pretraining; it will eventually go back down.)
		\begin{lstlisting}[basicstyle=\ttfamily, language=bash]
			# Pretrain the model
			python src/run.py pretrain vanilla wiki.txt \
			--writing_params_path vanilla.pretrain.params
			
			# Finetune the model
			python src/run.py finetune vanilla wiki.txt \
			--reading_params_path vanilla.pretrain.params \
			--writing_params_path vanilla.finetune.params \
			--finetune_corpus_path birth_places_train.tsv
			
			# Evaluate on the dev set; write to disk
			python src/run.py evaluate vanilla wiki.txt  \
			--reading_params_path vanilla.finetune.params \
			--eval_corpus_path birth_dev.tsv \
			--outputs_path vanilla.pretrain.dev.predictions
			
			# Evaluate on the test set; write to disk
			python src/run.py evaluate vanilla wiki.txt  \
			--reading_params_path vanilla.finetune.params \
			--eval_corpus_path birth_test_inputs.tsv \
			--outputs_path vanilla.pretrain.test.predictions
		\end{lstlisting}
		
		Report the accuracy on the dev set (printed by the third command above). We expect the dev accuracy will be at least 10\%, and will expect a similar accuracy on the held out test set.
		
		\textcolor{red}{\text{Answer:}Run procedure and results are shown in Fig. \ref{fig: 2.f.process.1}, \ref{fig: 2.f.result.1}.
		\begin{itemize}
			\item [$ \bullet $] dev accuracy: Correct: 85.0 out of 500.0: 17.0\%
		\end{itemize}
		\begin{figure}[htbp] 
			% read manual to see what [ht] means and for other possible options
			\centering 
			\includegraphics[width=0.9\linewidth]{picture/2.f.process.1}
			\captionsetup{font=small}
			\captionsetup{font=scriptsize}
			\caption{
				\label{fig: 2.f.process.1} % spaces are big no-no withing labels
				% things like fig: are optional in the label but it helps
				% to orient yourself when you have multiple figures,
				% equations and tables
				The output obtained from running python \texttt{src/run.py pretrain vanilla wiki.txt --writing\_params\_path vanilla.pretrain.params}
			}
		\end{figure}
		\begin{figure}[htbp] 
			% read manual to see what [ht] means and for other possible options
			\centering 
			\includegraphics[width=0.7\linewidth]{picture/2.f.result.1}
			\captionsetup{font=small}
			\captionsetup{font=scriptsize}
			\caption{
				\label{fig: 2.f.result.1} % spaces are big no-no withing labels
				% things like fig: are optional in the label but it helps
				% to orient yourself when you have multiple figures,
				% equations and tables
				The output obtained from running python \texttt{src/run.py evaluate vanilla wiki.txt --reading\_params\_path vanilla.finetune.params --eval\_corpus\_path birth\_dev.tsv --outputs\_path vanilla.pretrain.dev.predictions}
			}
		\end{figure}
		}
		
		\textcolor{blue}{\textbf{Changes:} Modified line 88-94 in \texttt{run.py}} 
		\begin{lstlisting}[language=python, basicstyle=\small\ttfamily]
			# line 88-94
			train_config = trainer.TrainerConfig(max_epochs=650, batch_size=128, \ 
					learning_rate=6e-3, lr_decay=True, \
					warmup_tokens=512 * 20, \
					final_tokens=200 * len(pretrain_dataset) * block_size, \
					num_workers=0)
			trainerGPT = trainer.Trainer(model, pretrain_dataset, None, train_config)
			trainerGPT.train()
			torch.save(model.state_dict(), args.writing_params_path)
		\end{lstlisting}
		
		
	\item[(g)]
		(10 points)\textbf{Research! Write and try out a more efficient variant of Attention (Budget 2 hours for pretraining!)}\\
		
		We'll now go to changing the Transformer architecture itself -- specifically the first and last transformer blocks.
		While we've been using a self-attention scoring function based on dot products, this involves a rather intensive computation that's quadratic in the sequence length. This is because the dot product between $\ell^2$ pairs of word vectors is computed in each computation. \textit{Synthesized attention} \cite{b3} is a very recent alternative that has potential benefits by removing this dot product (and quadratic computation) entirely. It's a promising idea, and one way for us to ask, "What's important/right about the Transformer architecture, and where can we improve/prune aspects of it?" In \texttt{attention.py}, implement the \texttt{forward()} method of \texttt{SynthesizerAttention}, which implements a variant of the Synthesizer proposed in the cited paper.
		
		The provided \texttt{CausalSelfAttention} layer implements the following attention for each head of the multi-headed attention: Let $X\in \mathbb{R}^{\ell \times d}$ (where $\ell$ is the block size and $d$ is the total dimensionality, $d/h$ is the dimensionality per head.).\footnote{Note that these dimensionalities do not include the minibatch dimension.}
		
		Let $Q_i,K_i,V_i \in \mathbb{R}^{d\times d/h}$.
		Then the output of the self-attention head is 
		\begin{equation} \label{qkv_eqn}
			Y_i = \text{softmax}\bigg(\frac{(XQ_i)(XK_i)^\top}{\sqrt{d/h}}\bigg)(XV_i)
		\end{equation}
		where $Y_i\in\mathbb{R}^{\ell \times d/h}$.
		Then the output of the self-attention is a linear transformation of the concatenation of the heads:
		\begin{equation}
			Y = [Y_1;\dots;Y_h]A
		\end{equation}
		where $A \in\mathbb{R}^{d\times d}$ and $[Y_1;\dots;Y_h]\in\mathbb{R}^{\ell \times d}$.
		The code also includes dropout layers which we haven't written here.
		We suggest looking at the provided code and noting how this equation is implemented in PyTorch.

		Your job is to implement the following variant of attention. Instead of Eq. \ref{qkv_eqn}, implement the
		following in SynthesizerAttention:
		\begin{equation}
			Y_i = \text{softmax}\left( \text{ReLU}(XA_i + b_1)B_i + b_2 \right)\left(XV_i\right),
		\end{equation}
		where $A_i \in \mathbf{R}^{d\times d/h}$, $B_i \in \mathbf{R}^{d/h \times \ell}$, and $V_i \in \mathbf{R}^{d \times d/h}$.\footnote{Hint: copy over the CausalSelfAttention class, and modify it minimally for this.} One way to interpret this is as follows: The term $(XQ_i)(XK_i)^\top$ is an $\ell \times \ell$ matrix of attention scores, computed as all pairs of dot products between word embeddings. The synthesizer variant eschews the all-pairs dot product and directly computes the $\ell \times \ell$ matrix of attention scores by mapping each d-dimensional vector of each head for $X$ to an $\ell$-dimesional vector of unnormalized attention weights.
		
		In the rest of the code in the \texttt{src}/ folder, modify your model to support using either \texttt{CausalSelfAttention} or \texttt{SynthesizerAttention}. Add the ability to switch between these attention variants depending on whether "vanilla" (for causal self-attention) or "synthesizer" (for the synthesizer variant) is selected in the command line arguments (see the section marked [part g] in \texttt{src/run.py}). You are free to implement this functionality in any way you choose, so long as it supports these command line arguments.
		
		Below are bash commands that your code should support in order to pretrain the model, finetune it, and make predictions on the dev and test sets.
		Note that the pretraining process will take approximately 2 hours. 
		\begin{lstlisting}[basicstyle=\ttfamily, language=bash]
			
			# Pretrain the model
			python src/run.py pretrain synthesizer wiki.txt \
					--writing_params_path synthesizer.pretrain.params
			
			# Finetune the model
			python src/run.py finetune synthesizer wiki.txt \
					--reading_params_path synthesizer.pretrain.params \
					--writing_params_path synthesizer.finetune.params \
					--finetune_corpus_path birth_places_train.tsv
			
			# Evaluate on the dev set; write to disk
			python src/run.py evaluate synthesizer wiki.txt \
					--reading_params_path synthesizer.finetune.params \
					--eval_corpus_path birth_dev.tsv \
					--outputs_path synthesizer.pretrain.dev.predictions
			
			# Evaluate on the test set; write to disk
			python src/run.py evaluate synthesizer wiki.txt \
					--reading_params_path synthesizer.finetune.params \
					--eval_corpus_path birth_test_inputs.tsv \
					--outputs_path synthesizer.pretrain.test. predictions
		\end{lstlisting}
		
		Report the accuracy of your perceiver attention model on birthplace prediction on \texttt{birth\_dev.tsv} after pretraining and fine-tuning.
		\begin{itemize}
		\item[i.]
			(8 points)We'll score your model as to whether it gets at least 5\% accuracy on the test set, which has answers held out.
			
			\textcolor{red}{\textbf{Answer:} dev accuracy: Correct: 87.0 out of 500.0: 17.4\%
			\begin{figure}[htbp] 
				% read manual to see what [ht] means and for other possible options
				\centering 
				\includegraphics[width=\linewidth]{picture/2.g.result}
				\captionsetup{font=small}
				%\captionsetup{font=scriptsize}
				\caption{
					\label{fig: 2.g.result} % spaces are big no-no withing labels
					% things like fig: are optional in the label but it helps
					% to orient yourself when you have multiple figures,
					% equations and tables
					The output obtained from running python \texttt{src/run.py evaluate synthesizer wiki.txt --reading\_params\_path synthesizer.finetune.params --eval\_corpus\_path birth\_dev.tsv --outputs\_path synthesizer.pretrain.dev.predictions
					}
				}
			\end{figure}
			}
			
			\textcolor{blue}{\textbf{Changes:} Modified line 62-64 in \texttt{run.py}}
			\begin{lstlisting}[language=python, basicstyle=\small\ttfamily]
				mconf.synthesizer = True
				model = model.GPT(mconf)
				model.to(device)
			\end{lstlisting}
			
		\item[ii.]
			(2 points)Why might the synthesizer self-attention not be able to do, in a single layer, what the key-query-value self-attention can do?
			
			\textcolor{red}{\text{Answer:} Synthesizer attention is unable to do what the causal attention can do because it does not extract keys and queries from the input - it estimates that by remapping input to a lower dimensional space for each head. In other words, it is more difficult to represent context because for every word in sequence, that word is unable to choose which parts to pay attention to in the rest of the sequence (because it does not have access to their keys). Thus the synthesizer may not be able to capture the relevance between word pairs.}
			
		\end{itemize}
		
	\end{itemize}
	
	
	\section{Considerations in pretrained knowledge (5 points)}
	
	\begin{itemize}
		\item[(a)]
			(1 point) Succinctly explain why the pretrained (vanilla) model was able to achieve an accuracy of above 10\%, whereas the non-pretrained model was not.
			
			\textcolor{red}{\text{Answer:} The pretrained (vanilla) model was able to achieve an accuracy above 10\% because it had been pretrained on a large corpus of text data, which helped it learn general language patterns and information. During pretraining, the model learned to predict the next word in a sentence based on the context it was given. This process allowed the model to develop a basic understanding of grammar, vocabulary, and semantic relationships. As a result, when fine-tuned on the specific task of predicting birthplaces, the pretrained model had some prior knowledge and linguistic abilities that helped it perform better than the non-pretrained model, which started from scratch.}
		\item[(b)]
			(2 points) Take a look at some of the correct predictions of the pretrain+finetuned vanilla model, as well as some of the errors. We think you'll find that it's impossible to tell, just looking at the output, whether the model retrieved the correct birth place, or made up an incorrect birth place. Consider the implications of this for user-facing systems that involve pretrained NLP components. Come up with two \textbf{distinct} reasons why this model behavior (i.e. unable to tell whether it's retrieved or made up) may cause concern for such applications, and an example for each reason.
			
			\textcolor{red}{\text{Answer:} 
			\begin{itemize}
				\item [$\bullet$] One reason why the inability to determine whether the model retrieved the correct birthplace or made up an incorrect one is concerning for user-facing systems is the potential for spreading misinformation. If the model generates incorrect birthplace predictions and presents them as factual information to users, it can lead to the dissemination of false or misleading data. For example, if the model inaccurately predicts a celebrity's birthplace, users relying on the system may unknowingly spread false information through social media or other platforms.
				\item [$\bullet$] Another concern is the erosion of trust in the system and its outputs. If users cannot discern whether the predicted birthplaces are reliable or fabricated, it undermines the credibility and usability of the system. Users may become skeptical or hesitant to rely on the system's predictions, especially if they have encountered instances where the predictions were incorrect. This lack of transparency and confidence in the model's outputs can significantly impact the user experience and limit the adoption of such applications.
			\end{itemize}	
			}
		\item[(c)] 
			(2 points) If your model didn't see a person's name at pretraining time, and that person was not seen at fine-tuning time either, it is not possible for it to have "learned" where they lived. Yet, your model will produce something as a predicted birth place for that person's name if asked. Concisely describe a strategy your model might take for predicting a birth place for that person's name, and one reason why this should cause concern for the use of such applications. (You do not need to submit the same answer for 3c as for 3b.)
			
			\textcolor{red}{\text{Answer:}
			\begin{itemize}
				\item [$\bullet$]
				In the absence of pretraining or fine-tuning data for a specific person's name, the model may employ a strategy of generating a birthplace based on statistical patterns observed in the training data. For example, the model might identify common patterns between names and birthplaces, such as geographical associations or cultural trends. It could use this information to make an educated guess about the person's birthplace.
				However, this strategy should raise concerns in the context of user-facing applications. One reason is the potential for perpetuating stereotypes or biases. If the model relies on statistical patterns that reflect existing societal biases, it may produce birthplace predictions that align with those biases, reinforcing stereotypes or prejudices. This can have negative implications, such as perpetuating discriminatory views or reinforcing unfair generalizations based on demographics.
				\item [$\bullet$]
				Additionally, generating birthplace predictions for individuals without proper data or evidence can lead to the spread of speculative or misleading information. Users may interpret the model's predictions as factual, even if they are based on assumptions or statistical correlations. This can contribute to the proliferation of inaccurate or unverified information, further complicating the task of distinguishing between reliable and unreliable sources of information in user-facing systems.
			\end{itemize}
			}
	\end{itemize}

		
\renewcommand{\refname}{References}
		
\begin{thebibliography}{00}
			
	\bibitem{b1}\label{cite:b1}
		Radford, A., Narasimhan, K., Salimans, T., and Sutskever, I. Improving language understanding with unsupervised learning. Technical report, OpenAI (2018).
			
	\bibitem{b2}\label{cite:b2}
		Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research 21, 140 (2020), 1–67.
			
	\bibitem{b3}\label{cite:b3}
		Tay, Y., Bahri, D., Metzler, D., Juan, D.-C., Zhao, Z., and Zheng, C. Synthesizer: Rethinking self-attention in transformer models. arXiv preprint arXiv:2005.00743 (2020).
			
			
			
\end{thebibliography}
		
%		\bibliographystyle{unsrt}
%		\bibliography{reference}
		
		
	\end{document}
